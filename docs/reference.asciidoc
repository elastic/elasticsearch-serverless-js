[[api-reference]]
////////
===========================================================================================================================
||                                                                                                                       ||
||                                                                                                                       ||
||                                                                                                                       ||
||        ██████╗ ███████╗ █████╗ ██████╗ ███╗   ███╗███████╗                                                            ||
||        ██╔══██╗██╔════╝██╔══██╗██╔══██╗████╗ ████║██╔════╝                                                            ||
||        ██████╔╝█████╗  ███████║██║  ██║██╔████╔██║█████╗                                                              ||
||        ██╔══██╗██╔══╝  ██╔══██║██║  ██║██║╚██╔╝██║██╔══╝                                                              ||
||        ██║  ██║███████╗██║  ██║██████╔╝██║ ╚═╝ ██║███████╗                                                            ||
||        ╚═╝  ╚═╝╚══════╝╚═╝  ╚═╝╚═════╝ ╚═╝     ╚═╝╚══════╝                                                            ||
||                                                                                                                       ||
||                                                                                                                       ||
||    This file is autogenerated, DO NOT send pull requests that changes this file directly.                             ||
||    You should update the script that does the generation, which can be found in:                                      ||
||    https://github.com/elastic/elastic-client-generator-js                                                             ||
||                                                                                                                       ||
||    You can run the script with the following command:                                                                 ||
||       npm run elasticsearch -- --version <version>                                                                    ||
||                                                                                                                       ||
||                                                                                                                       ||
||                                                                                                                       ||
===========================================================================================================================
////////
== API Reference

[discrete]
=== bulk
Allows to perform multiple index/update/delete operations in a single request.

{ref}/docs-bulk.html[Endpoint documentation]
[source,ts]
----
client.bulk({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Name of the data stream, index, or index alias to perform bulk actions on.
** *`operations` (Optional, { index, create, update, delete } | { detect_noop, doc, doc_as_upsert, script, scripted_upsert, _source, upsert } | object[])*
** *`pipeline` (Optional, string)*: ID of the pipeline to use to preprocess incoming documents.
If the index has a default ingest pipeline specified, then setting the value to `_none` disables the default ingest pipeline for this request.
If a final pipeline is configured it will always run, regardless of the value of this parameter.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true`, Elasticsearch refreshes the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` do nothing with refreshes.
Valid values: `true`, `false`, `wait_for`.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`_source` (Optional, boolean | string | string[])*: `true` or `false` to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude from the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`timeout` (Optional, string | -1 | 0)*: Period each action waits for the following operations: automatic index creation, dynamic mapping updates, waiting for active shards.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`require_alias` (Optional, boolean)*: If `true`, the request’s actions must target an index alias.

[discrete]
=== clear_scroll
Explicitly clears the search context for a scroll.

{ref}/clear-scroll-api.html[Endpoint documentation]
[source,ts]
----
client.clearScroll({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`scroll_id` (Optional, string | string[])*: List of scroll IDs to clear.
To clear all scroll IDs, use `_all`.

[discrete]
=== close_point_in_time
Close a point in time

{ref}/point-in-time-api.html[Endpoint documentation]
[source,ts]
----
client.closePointInTime({ id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The ID of the point-in-time.

[discrete]
=== count
Returns number of documents matching a query.

{ref}/search-count.html[Endpoint documentation]
[source,ts]
----
client.count({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams and indices, omit this parameter or use `*` or `_all`.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
This parameter can only be used when the `q` query string parameter is specified.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
This parameter can only be used when the `q` query string parameter is specified.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
** *`ignore_throttled` (Optional, boolean)*: If `true`, concrete, expanded or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`min_score` (Optional, number)*: Sets the minimum `_score` value that documents must have to be included in the result.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard.
If a query reaches this limit, Elasticsearch terminates the query early.
Elasticsearch collects documents before sorting.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.

[discrete]
=== create
Creates a new document in the index.

Returns a 409 response when a document with a same ID already exists in the index.

{ref}/docs-index_.html[Endpoint documentation]
[source,ts]
----
client.create({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Unique identifier for the document.
** *`index` (string)*: Name of the data stream or index to target.
If the target doesn’t exist and matches the name or wildcard (`*`) pattern of an index template with a `data_stream` definition, this request creates the data stream.
If the target doesn’t exist and doesn’t match a data stream template, this request creates the index.
** *`document` (Optional, object)*: A document.
** *`pipeline` (Optional, string)*: ID of the pipeline to use to preprocess incoming documents.
If the index has a default ingest pipeline specified, then setting the value to `_none` disables the default ingest pipeline for this request.
If a final pipeline is configured it will always run, regardless of the value of this parameter.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true`, Elasticsearch refreshes the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` do nothing with refreshes.
Valid values: `true`, `false`, `wait_for`.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Period the request waits for the following operations: automatic index creation, dynamic mapping updates, waiting for active shards.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
=== delete
Removes a document from the index.

{ref}/docs-delete.html[Endpoint documentation]
[source,ts]
----
client.delete({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Unique identifier for the document.
** *`index` (string)*: Name of the target index.
** *`if_primary_term` (Optional, number)*: Only perform the operation if the document has this primary term.
** *`if_seq_no` (Optional, number)*: Only perform the operation if the document has this sequence number.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true`, Elasticsearch refreshes the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` do nothing with refreshes.
Valid values: `true`, `false`, `wait_for`.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for active shards.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
=== delete_by_query
Deletes documents matching the provided query.

{ref}/docs-delete-by-query.html[Endpoint documentation]
[source,ts]
----
client.deleteByQuery({ index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams or indices, omit this parameter or use `*` or `_all`.
** *`max_docs` (Optional, number)*: The maximum number of documents to delete.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Specifies the documents to delete using the Query DSL.
** *`slice` (Optional, { field, id, max })*: Slice the request manually using the provided slice ID and total number of slices.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
** *`conflicts` (Optional, Enum("abort" | "proceed"))*: What to do if delete by query hits version conflicts: `abort` or `proceed`.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`. Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`from` (Optional, number)*: Starting offset (default: 0)
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`refresh` (Optional, boolean)*: If `true`, Elasticsearch refreshes all shards involved in the delete by query after the request completes.
** *`request_cache` (Optional, boolean)*: If `true`, the request cache is used for this request.
Defaults to the index-level setting.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.
** *`scroll` (Optional, string | -1 | 0)*: Period to retain the search context for scrolling.
** *`scroll_size` (Optional, number)*: Size of the scroll request that powers the operation.
** *`search_timeout` (Optional, string | -1 | 0)*: Explicit timeout for each search request.
Defaults to no timeout.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: The type of the search operation.
Available options: `query_then_fetch`, `dfs_query_then_fetch`.
** *`slices` (Optional, number | Enum("auto"))*: The number of slices this task should be divided into.
** *`sort` (Optional, string[])*: A list of <field>:<direction> pairs.
** *`stats` (Optional, string[])*: Specific `tag` of the request for logging and statistical purposes.
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard.
If a query reaches this limit, Elasticsearch terminates the query early.
Elasticsearch collects documents before sorting.
Use with caution.
Elasticsearch applies this parameter to each shard handling the request.
When possible, let Elasticsearch perform early termination automatically.
Avoid specifying this parameter for requests that target data streams with backing indices across multiple data tiers.
** *`timeout` (Optional, string | -1 | 0)*: Period each deletion request waits for active shards.
** *`version` (Optional, boolean)*: If `true`, returns the document version as part of a hit.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the operation is complete.

[discrete]
=== delete_script
Deletes a script.

{ref}/modules-scripting.html[Endpoint documentation]
[source,ts]
----
client.deleteScript({ id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the stored script or search template.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
=== exists
Returns information about whether a document exists in an index.

{ref}/docs-get.html[Endpoint documentation]
[source,ts]
----
client.exists({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier of the document.
** *`index` (string)*: List of data streams, indices, and aliases.
Supports wildcards (`*`).
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`realtime` (Optional, boolean)*: If `true`, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If `true`, Elasticsearch refreshes all shards involved in the delete by query after the request completes.
** *`routing` (Optional, string)*: Target the specified primary shard.
** *`_source` (Optional, boolean | string | string[])*: `true` or `false` to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude in the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit.
If no fields are specified, no stored fields are included in the response.
If this field is specified, the `_source` parameter defaults to false.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.

[discrete]
=== exists_source
Returns information about whether a document source exists in an index.

{ref}/docs-get.html[Endpoint documentation]
[source,ts]
----
client.existsSource({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier of the document.
** *`index` (string)*: List of data streams, indices, and aliases.
Supports wildcards (`*`).
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`realtime` (Optional, boolean)*: If true, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If `true`, Elasticsearch refreshes all shards involved in the delete by query after the request completes.
** *`routing` (Optional, string)*: Target the specified primary shard.
** *`_source` (Optional, boolean | string | string[])*: `true` or `false` to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude in the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.

[discrete]
=== explain
Returns information about why a specific matches (or doesn't match) a query.

{ref}/search-explain.html[Endpoint documentation]
[source,ts]
----
client.explain({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Defines the document ID.
** *`index` (string)*: Index names used to limit the request.
Only a single index name can be provided to this parameter.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`_source` (Optional, boolean | string | string[])*: True or false to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude from the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`stored_fields` (Optional, string | string[])*: A list of stored fields to return in the response.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.

[discrete]
=== field_caps
Returns the information about the capabilities of fields among multiple indices.

{ref}/search-field-caps.html[Endpoint documentation]
[source,ts]
----
client.fieldCaps({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request. Supports wildcards (*). To target all data streams and indices, omit this parameter or use * or _all.
** *`index_filter` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Allows to filter indices if the provided query rewrites to match_none on every shard.
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias,
or `_all` value targets only missing or closed indices. This behavior applies even if the request targets other open indices. For example, a request
targeting `foo*,bar*` returns an error if an index starts with foo but no index starts with bar.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams. Supports a list of values, such as `open,hidden`.
** *`fields` (Optional, string | string[])*: List of fields to retrieve capabilities for. Wildcard (`*`) expressions are supported.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, missing or closed indices are not included in the response.
** *`include_unmapped` (Optional, boolean)*: If true, unmapped fields are included in the response.

[discrete]
=== get
Returns a document.

{ref}/docs-get.html[Endpoint documentation]
[source,ts]
----
client.get({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Unique identifier of the document.
** *`index` (string)*: Name of the index that contains the document.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on. Random by default.
** *`realtime` (Optional, boolean)*: If `true`, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If true, Elasticsearch refreshes the affected shards to make this operation visible to search. If false, do nothing with refreshes.
** *`routing` (Optional, string)*: Target the specified primary shard.
** *`_source` (Optional, boolean | string | string[])*: True or false to return the _source field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude in the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit.
If no fields are specified, no stored fields are included in the response.
If this field is specified, the `_source` parameter defaults to false.
** *`version` (Optional, number)*: Explicit version number for concurrency control. The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: internal, external, external_gte.

[discrete]
=== get_script
Returns a script.

{ref}/modules-scripting.html[Endpoint documentation]
[source,ts]
----
client.getScript({ id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the stored script or search template.
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master

[discrete]
=== get_source
Returns the source of a document.

{ref}/docs-get.html[Endpoint documentation]
[source,ts]
----
client.getSource({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Unique identifier of the document.
** *`index` (string)*: Name of the index that contains the document.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on. Random by default.
** *`realtime` (Optional, boolean)*: Boolean) If true, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If true, Elasticsearch refreshes the affected shards to make this operation visible to search. If false, do nothing with refreshes.
** *`routing` (Optional, string)*: Target the specified primary shard.
** *`_source` (Optional, boolean | string | string[])*: True or false to return the _source field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude in the response.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
** *`stored_fields` (Optional, string | string[])*
** *`version` (Optional, number)*: Explicit version number for concurrency control. The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: internal, external, external_gte.

[discrete]
=== index
Creates or updates a document in an index.

{ref}/docs-index_.html[Endpoint documentation]
[source,ts]
----
client.index({ index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the data stream or index to target.
** *`id` (Optional, string)*: Unique identifier for the document.
** *`document` (Optional, object)*: A document.
** *`if_primary_term` (Optional, number)*: Only perform the operation if the document has this primary term.
** *`if_seq_no` (Optional, number)*: Only perform the operation if the document has this sequence number.
** *`op_type` (Optional, Enum("index" | "create"))*: Set to create to only index the document if it does not already exist (put if absent).
If a document with the specified `_id` already exists, the indexing operation will fail.
Same as using the `<index>/_create` endpoint.
Valid values: `index`, `create`.
If document id is specified, it defaults to `index`.
Otherwise, it defaults to `create`.
** *`pipeline` (Optional, string)*: ID of the pipeline to use to preprocess incoming documents.
If the index has a default ingest pipeline specified, then setting the value to `_none` disables the default ingest pipeline for this request.
If a final pipeline is configured it will always run, regardless of the value of this parameter.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true`, Elasticsearch refreshes the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` do nothing with refreshes.
Valid values: `true`, `false`, `wait_for`.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Period the request waits for the following operations: automatic index creation, dynamic mapping updates, waiting for active shards.
** *`version` (Optional, number)*: Explicit version number for concurrency control.
The specified version must match the current version of the document for the request to succeed.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type: `external`, `external_gte`.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`require_alias` (Optional, boolean)*: If `true`, the destination must be an index alias.

[discrete]
=== info
Returns basic information about the cluster.

{ref}/index.html[Endpoint documentation]
[source,ts]
----
client.info()
----

[discrete]
=== mget
Allows to get multiple documents in one request.

{ref}/docs-multi-get.html[Endpoint documentation]
[source,ts]
----
client.mget({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Name of the index to retrieve documents from when `ids` are specified, or when a document in the `docs` array does not specify an index.
** *`docs` (Optional, { _id, _index, routing, _source, stored_fields, version, version_type }[])*: The documents you want to retrieve. Required if no index is specified in the request URI.
** *`ids` (Optional, string | string[])*: The IDs of the documents you want to retrieve. Allowed when the index is specified in the request URI.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on. Random by default.
** *`realtime` (Optional, boolean)*: If `true`, the request is real-time as opposed to near-real-time.
** *`refresh` (Optional, boolean)*: If `true`, the request refreshes relevant shards before retrieving documents.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`_source` (Optional, boolean | string | string[])*: True or false to return the `_source` field or not, or a list of fields to return.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude from the response.
You can also use this parameter to exclude fields from the subset specified in `_source_includes` query parameter.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
If this parameter is specified, only these source fields are returned. You can exclude fields from this subset using the `_source_excludes` query parameter.
If the `_source` parameter is `false`, this parameter is ignored.
** *`stored_fields` (Optional, string | string[])*: If `true`, retrieves the document fields stored in the index rather than the document `_source`.

[discrete]
=== msearch
Allows to execute several search operations in one request.

{ref}/search-multi-search.html[Endpoint documentation]
[source,ts]
----
client.msearch({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and index aliases to search.
** *`searches` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, index, preference, request_cache, routing, search_type, ccs_minimize_roundtrips, allow_partial_search_results, ignore_throttled } | { aggregations, collapse, query, explain, ext, stored_fields, docvalue_fields, from, highlight, indices_boost, min_score, post_filter, profile, rescore, script_fields, search_after, size, sort, _source, fields, terminate_after, stats, timeout, track_scores, track_total_hits, version, runtime_mappings, seq_no_primary_term, pit, suggest }[])*
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias, or _all value targets only missing or closed indices. This behavior applies even if the request targets other open indices. For example, a request targeting foo*,bar* returns an error if an index starts with foo but no index starts with bar.
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If true, network roundtrips between the coordinating node and remote clusters are minimized for cross-cluster search requests.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard expressions can match. If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
** *`ignore_throttled` (Optional, boolean)*: If true, concrete, expanded or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If true, missing or closed indices are not included in the response.
** *`max_concurrent_searches` (Optional, number)*: Maximum number of concurrent searches the multi search API can execute.
** *`max_concurrent_shard_requests` (Optional, number)*: Maximum number of concurrent shard requests that each sub-search request executes per node.
** *`pre_filter_shard_size` (Optional, number)*: Defines a threshold that enforces a pre-filter roundtrip to prefilter search shards based on query rewriting if the number of shards the search request expands to exceeds the threshold. This filter roundtrip can limit the number of shards significantly if for instance a shard can not match any documents based on its rewrite method i.e., if date filters are mandatory to match but the shard bounds and the query are disjoint.
** *`rest_total_hits_as_int` (Optional, boolean)*: If true, hits.total are returned as an integer in the response. Defaults to false, which returns an object.
** *`routing` (Optional, string)*: Custom routing value used to route search operations to a specific shard.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: Indicates whether global term and document frequencies should be used when scoring returned documents.
** *`typed_keys` (Optional, boolean)*: Specifies whether aggregation and suggester names should be prefixed by their respective types in the response.

[discrete]
=== msearch_template
Allows to execute several search template operations in one request.

{ref}/search-multi-search.html[Endpoint documentation]
[source,ts]
----
client.msearchTemplate({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams and indices, omit this parameter or use `*`.
** *`search_templates` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, index, preference, request_cache, routing, search_type, ccs_minimize_roundtrips, allow_partial_search_results, ignore_throttled } | { aggregations, collapse, query, explain, ext, stored_fields, docvalue_fields, from, highlight, indices_boost, min_score, post_filter, profile, rescore, script_fields, search_after, size, sort, _source, fields, terminate_after, stats, timeout, track_scores, track_total_hits, version, runtime_mappings, seq_no_primary_term, pit, suggest }[])*
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If `true`, network round-trips are minimized for cross-cluster search requests.
** *`max_concurrent_searches` (Optional, number)*: Maximum number of concurrent searches the API can run.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: The type of the search operation.
Available options: `query_then_fetch`, `dfs_query_then_fetch`.
** *`rest_total_hits_as_int` (Optional, boolean)*: If `true`, the response returns `hits.total` as an integer.
If `false`, it returns `hits.total` as an object.
** *`typed_keys` (Optional, boolean)*: If `true`, the response prefixes aggregation and suggester names with their respective types.

[discrete]
=== mtermvectors
Returns multiple termvectors in one request.

{ref}/docs-multi-termvectors.html[Endpoint documentation]
[source,ts]
----
client.mtermvectors({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Name of the index that contains the documents.
** *`docs` (Optional, { _id, _index, routing, _source, stored_fields, version, version_type }[])*: Array of existing or artificial documents.
** *`ids` (Optional, string[])*: Simplified syntax to specify documents by their ID if they're in the same index.
** *`fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in the statistics.
Used as the default list unless a specific field list is provided in the `completion_fields` or `fielddata_fields` parameters.
** *`field_statistics` (Optional, boolean)*: If `true`, the response includes the document count, sum of document frequencies, and sum of total term frequencies.
** *`offsets` (Optional, boolean)*: If `true`, the response includes term offsets.
** *`payloads` (Optional, boolean)*: If `true`, the response includes term payloads.
** *`positions` (Optional, boolean)*: If `true`, the response includes term positions.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`realtime` (Optional, boolean)*: If true, the request is real-time as opposed to near-real-time.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`term_statistics` (Optional, boolean)*: If true, the response includes term frequency and document frequency.
** *`version` (Optional, number)*: If `true`, returns the document version as part of a hit.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type.

[discrete]
=== open_point_in_time
Open a point in time that can be used in subsequent searches

{ref}/point-in-time-api.html[Endpoint documentation]
[source,ts]
----
client.openPointInTime({ index, keep_alive })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: A list of index names to open point in time; use `_all` or empty string to perform the operation on all indices
** *`keep_alive` (string | -1 | 0)*: Extends the time to live of the corresponding point in time.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`. Valid values are: `all`, `open`, `closed`, `hidden`, `none`.

[discrete]
=== ping
Returns whether the cluster is running.

{ref}/index.html[Endpoint documentation]
[source,ts]
----
client.ping()
----

[discrete]
=== put_script
Creates or updates a script.

{ref}/modules-scripting.html[Endpoint documentation]
[source,ts]
----
client.putScript({ id, script })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the stored script or search template.
Must be unique within the cluster.
** *`script` ({ lang, options, source })*: Contains the script or search template, its parameters, and its language.
** *`context` (Optional, string)*: Context in which the script or search template should run.
To prevent errors, the API immediately compiles the script or template in this context.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
=== rank_eval
Allows to evaluate the quality of ranked search results over a set of typical search queries

{ref}/search-rank-eval.html[Endpoint documentation]
[source,ts]
----
client.rankEval({ requests })
----
[discrete]
==== Arguments

* *Request (object):*
** *`requests` ({ id, request, ratings, template_id, params }[])*: A set of typical search requests, together with their provided ratings.
** *`index` (Optional, string | string[])*: List of data streams, indices, and index aliases used to limit the request. Wildcard (`*`) expressions are supported.
To target all data streams and indices in a cluster, omit this parameter or use `_all` or `*`.
** *`metric` (Optional, { precision, recall, mean_reciprocal_rank, dcg, expected_reciprocal_rank })*: Definition of the evaluation metric to calculate.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices. This behavior applies even if the request targets other open indices. For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, missing or closed indices are not included in the response.
** *`search_type` (Optional, string)*: Search operation type

[discrete]
=== reindex
Allows to copy documents from one index to another, optionally filtering the source
documents by a query, changing the destination index settings, or fetching the
documents from a remote cluster.

{ref}/docs-reindex.html[Endpoint documentation]
[source,ts]
----
client.reindex({ dest, source })
----
[discrete]
==== Arguments

* *Request (object):*
** *`dest` ({ index, op_type, pipeline, routing, version_type })*: The destination you are copying to.
** *`source` ({ index, query, remote, size, slice, sort, _source, runtime_mappings })*: The source you are copying from.
** *`conflicts` (Optional, Enum("abort" | "proceed"))*: Set to proceed to continue reindexing even if there are conflicts.
** *`max_docs` (Optional, number)*: The maximum number of documents to reindex.
** *`script` (Optional, { lang, options, source } | { id })*: The script to run to update the document source or metadata when reindexing.
** *`size` (Optional, number)*
** *`refresh` (Optional, boolean)*: If `true`, the request refreshes affected shards to make this operation visible to search.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.
Defaults to no throttle.
** *`scroll` (Optional, string | -1 | 0)*: Specifies how long a consistent view of the index should be maintained for scrolled search.
** *`slices` (Optional, number | Enum("auto"))*: The number of slices this task should be divided into.
Defaults to 1 slice, meaning the task isn’t sliced into subtasks.
** *`timeout` (Optional, string | -1 | 0)*: Period each indexing waits for automatic index creation, dynamic mapping updates, and waiting for active shards.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the operation is complete.
** *`require_alias` (Optional, boolean)*: If `true`, the destination must be an index alias.

[discrete]
=== render_search_template
Allows to use the Mustache language to pre-render a search definition.

{ref}/render-search-template-api.html[Endpoint documentation]
[source,ts]
----
client.renderSearchTemplate({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: ID of the search template to render.
If no `source` is specified, this or the `id` request body parameter is required.
** *`file` (Optional, string)*
** *`params` (Optional, Record<string, User-defined value>)*: Key-value pairs used to replace Mustache variables in the template.
The key is the variable name.
The value is the variable value.
** *`source` (Optional, string)*: An inline search template.
Supports the same parameters as the search API's request body.
These parameters also support Mustache variables.
If no `id` or `<templated-id>` is specified, this parameter is required.

[discrete]
=== scripts_painless_execute
Allows an arbitrary script to be executed and a result to be returned

{painless}/painless-execute-api.html[Endpoint documentation]
[source,ts]
----
client.scriptsPainlessExecute({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`context` (Optional, string)*: The context that the script should run in.
** *`context_setup` (Optional, { document, index, query })*: Additional parameters for the `context`.
** *`script` (Optional, { lang, options, source })*: The Painless script to execute.

[discrete]
=== scroll
Allows to retrieve a large numbers of results from a single search request.

{ref}/search-request-body.html[Endpoint documentation]
[source,ts]
----
client.scroll({ scroll_id })
----
[discrete]
==== Arguments

* *Request (object):*
** *`scroll_id` (string)*: Scroll ID of the search.
** *`scroll` (Optional, string | -1 | 0)*: Period to retain the search context for scrolling.
** *`rest_total_hits_as_int` (Optional, boolean)*: If true, the API response’s hit.total property is returned as an integer. If false, the API response’s hit.total property is returned as an object.

[discrete]
=== search
Returns results matching a query.

{ref}/search-search.html[Endpoint documentation]
[source,ts]
----
client.search({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams and indices, omit this parameter or use `*` or `_all`.
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, cardinality, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: Defines the aggregations that are run as part of the search request.
** *`collapse` (Optional, { field, inner_hits, max_concurrent_group_searches, collapse })*: Collapses search results the values of the specified field.
** *`explain` (Optional, boolean)*: If true, returns detailed information about score computation as part of a hit.
** *`ext` (Optional, Record<string, User-defined value>)*: Configuration of search extensions defined by Elasticsearch plugins.
** *`from` (Optional, number)*: Starting document offset.
Needs to be non-negative.
By default, you cannot page through more than 10,000 hits using the `from` and `size` parameters.
To page through more hits, use the `search_after` parameter.
** *`highlight` (Optional, { encoder, fields })*: Specifies the highlighter to use for retrieving highlighted snippets from one or more fields in your search results.
** *`track_total_hits` (Optional, boolean | number)*: Number of hits matching the query to count accurately.
If `true`, the exact number of hits is returned at the cost of some performance.
If `false`, the  response does not include the total number of hits matching the query.
** *`indices_boost` (Optional, Record<string, number>[])*: Boosts the _score of documents from specified indices.
** *`docvalue_fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (`*`) patterns.
The request returns doc values for field names matching these patterns in the `hits.fields` property of the response.
** *`min_score` (Optional, number)*: Minimum `_score` for matching documents.
Documents with a lower `_score` are not included in the search results.
** *`post_filter` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Use the `post_filter` parameter to filter search results.
The search hits are filtered after the aggregations are calculated.
A post filter has no impact on the aggregation results.
** *`profile` (Optional, boolean)*: Set to `true` to return detailed timing information about the execution of individual components in a search request.
NOTE: This is a debugging tool and adds significant overhead to search execution.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`rescore` (Optional, { query, window_size } | { query, window_size }[])*: Can be used to improve precision by reordering just the top (for example 100 - 500) documents returned by the `query` and `post_filter` phases.
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Retrieve a script evaluation (based on different fields) for each hit.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*: Used to retrieve the next page of hits using a set of sort values from the previous page.
** *`size` (Optional, number)*: The number of hits to return.
By default, you cannot page through more than 10,000 hits using the `from` and `size` parameters.
To page through more hits, use the `search_after` parameter.
** *`slice` (Optional, { field, id, max })*: Can be used to split a scrolled search into multiple slices that can be consumed independently.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: A list of <field>:<direction> pairs.
** *`_source` (Optional, boolean | { excludes, includes })*: Indicates which source fields are returned for matching documents.
These fields are returned in the hits._source property of the search response.
** *`fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (`*`) patterns.
The request returns values for field names matching these patterns in the `hits.fields` property of the response.
** *`suggest` (Optional, { text })*: Defines a suggester that provides similar looking terms based on a provided text.
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard.
If a query reaches this limit, Elasticsearch terminates the query early.
Elasticsearch collects documents before sorting.
Use with caution.
Elasticsearch applies this parameter to each shard handling the request.
When possible, let Elasticsearch perform early termination automatically.
Avoid specifying this parameter for requests that target data streams with backing indices across multiple data tiers.
If set to `0` (default), the query does not terminate early.
** *`timeout` (Optional, string)*: Specifies the period of time to wait for a response from each shard.
If no response is received before the timeout expires, the request fails and returns an error.
Defaults to no timeout.
** *`track_scores` (Optional, boolean)*: If true, calculate and return document scores, even if the scores are not used for sorting.
** *`version` (Optional, boolean)*: If true, returns document version as part of a hit.
** *`seq_no_primary_term` (Optional, boolean)*: If `true`, returns sequence number and primary term of the last modification of each hit.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit.
If no fields are specified, no stored fields are included in the response.
If this field is specified, the `_source` parameter defaults to `false`.
You can pass `_source: true` to return both source fields and stored fields in the search response.
** *`pit` (Optional, { id, keep_alive })*: Limits the search to a point in time (PIT).
If you provide a PIT, you cannot specify an `<index>` in the request path.
** *`runtime_mappings` (Optional, Record<string, { fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request.
These fields take precedence over mapped fields with the same name.
** *`stats` (Optional, string[])*: Stats groups to associate with the search.
Each group maintains a statistics aggregation for its associated searches.
You can retrieve these stats using the indices stats API.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`allow_partial_search_results` (Optional, boolean)*: If true, returns partial results if there are shard request timeouts or shard failures. If false, returns an error with no partial results.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
This parameter can only be used when the q query string parameter is specified.
** *`analyze_wildcard` (Optional, boolean)*: If true, wildcard and prefix queries are analyzed.
This parameter can only be used when the q query string parameter is specified.
** *`batched_reduce_size` (Optional, number)*: The number of shard results that should be reduced at once on the coordinating node.
This value should be used as a protection mechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If true, network round-trips between the coordinating node and the remote clusters are minimized when executing cross-cluster search (CCS) requests.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: AND or OR.
This parameter can only be used when the `q` query string parameter is specified.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
This parameter can only be used when the q query string parameter is specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
** *`ignore_throttled` (Optional, boolean)*: If `true`, concrete, expanded or aliased indices will be ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
This parameter can only be used when the `q` query string parameter is specified.
** *`max_concurrent_shard_requests` (Optional, number)*: Defines the number of concurrent shard requests per node this search executes concurrently.
This value should be used to limit the impact of the search on the cluster in order to limit the number of concurrent shard requests.
** *`min_compatible_shard_node` (Optional, string)*: The minimum version of the node that can handle the request
Any handling node with a lower version will fail the request.
** *`preference` (Optional, string)*: Nodes and shards used for the search.
By default, Elasticsearch selects from eligible nodes and shards using adaptive replica selection, accounting for allocation awareness. Valid values are:
`_only_local` to run the search only on shards on the local node;
`_local` to, if possible, run the search on shards on the local node, or if not, select shards using the default method;
`_only_nodes:<node-id>,<node-id>` to run the search on only the specified nodes IDs, where, if suitable shards exist on more than one selected node, use shards on those nodes using the default method, or if none of the specified nodes are available, select shards from any available node using the default method;
`_prefer_nodes:<node-id>,<node-id>` to if possible, run the search on the specified nodes IDs, or if not, select shards using the default method;
`_shards:<shard>,<shard>` to run the search only on the specified shards;
`<custom-string>` (any string that does not start with `_`) to route searches with the same `<custom-string>` to the same shards in the same order.
** *`pre_filter_shard_size` (Optional, number)*: Defines a threshold that enforces a pre-filter roundtrip to prefilter search shards based on query rewriting if the number of shards the search request expands to exceeds the threshold.
This filter roundtrip can limit the number of shards significantly if for instance a shard can not match any documents based on its rewrite method (if date filters are mandatory to match but the shard bounds and the query are disjoint).
When unspecified, the pre-filter phase is executed if any of these conditions is met:
the request targets more than 128 shards;
the request targets one or more read-only index;
the primary sort of the query targets an indexed field.
** *`request_cache` (Optional, boolean)*: If `true`, the caching of search results is enabled for requests where `size` is `0`.
Defaults to index level settings.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`scroll` (Optional, string | -1 | 0)*: Period to retain the search context for scrolling. See Scroll search results.
By default, this value cannot exceed `1d` (24 hours).
You can change this limit using the `search.max_keep_alive` cluster-level setting.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: How distributed term frequencies are calculated for relevance scoring.
** *`suggest_field` (Optional, string)*: Specifies which field to use for suggestions.
** *`suggest_mode` (Optional, Enum("missing" | "popular" | "always"))*: Specifies the suggest mode.
This parameter can only be used when the `suggest_field` and `suggest_text` query string parameters are specified.
** *`suggest_size` (Optional, number)*: Number of suggestions to return.
This parameter can only be used when the `suggest_field` and `suggest_text` query string parameters are specified.
** *`suggest_text` (Optional, string)*: The source text for which the suggestions should be returned.
This parameter can only be used when the `suggest_field` and `suggest_text` query string parameters are specified.
** *`typed_keys` (Optional, boolean)*: If `true`, aggregation and suggester names are be prefixed by their respective types in the response.
** *`rest_total_hits_as_int` (Optional, boolean)*: Indicates whether `hits.total` should be rendered as an integer or an object in the rest search response.
** *`_source_excludes` (Optional, string | string[])*: A list of source fields to exclude from the response.
You can also use this parameter to exclude fields from the subset specified in `_source_includes` query parameter.
If the `_source` parameter is `false`, this parameter is ignored.
** *`_source_includes` (Optional, string | string[])*: A list of source fields to include in the response.
If this parameter is specified, only these source fields are returned.
You can exclude fields from this subset using the `_source_excludes` query parameter.
If the `_source` parameter is `false`, this parameter is ignored.
** *`q` (Optional, string)*: Query in the Lucene query string syntax using query parameter search.
Query parameter searches do not support the full Elasticsearch Query DSL but are handy for testing.

[discrete]
=== search_mvt
Searches a vector tile for geospatial values. Returns results as a binary Mapbox vector tile.

{ref}/search-vector-tile-api.html[Endpoint documentation]
[source,ts]
----
client.searchMvt({ index, field, zoom, x, y })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, or aliases to search
** *`field` (string)*: Field containing geospatial data to return
** *`zoom` (number)*: Zoom level for the vector tile to search
** *`x` (number)*: X coordinate for the vector tile to search
** *`y` (number)*: Y coordinate for the vector tile to search
** *`aggs` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, cardinality, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: Sub-aggregations for the geotile_grid.

Supports the following aggregation types:
- avg
- cardinality
- max
- min
- sum
** *`buffer` (Optional, number)*: Size, in pixels, of a clipping buffer outside the tile. This allows renderers
to avoid outline artifacts from geometries that extend past the extent of the tile.
** *`exact_bounds` (Optional, boolean)*: If false, the meta layer’s feature is the bounding box of the tile.
If true, the meta layer’s feature is a bounding box resulting from a
geo_bounds aggregation. The aggregation runs on <field> values that intersect
the <zoom>/<x>/<y> tile with wrap_longitude set to false. The resulting
bounding box may be larger than the vector tile.
** *`extent` (Optional, number)*: Size, in pixels, of a side of the tile. Vector tiles are square with equal sides.
** *`fields` (Optional, string | string[])*: Fields to return in the `hits` layer. Supports wildcards (`*`).
This parameter does not support fields with array values. Fields with array
values may return inconsistent results.
** *`grid_agg` (Optional, Enum("geotile" | "geohex"))*: Aggregation used to create a grid for the `field`.
** *`grid_precision` (Optional, number)*: Additional zoom levels available through the aggs layer. For example, if <zoom> is 7
and grid_precision is 8, you can zoom in up to level 15. Accepts 0-8. If 0, results
don’t include the aggs layer.
** *`grid_type` (Optional, Enum("grid" | "point" | "centroid"))*: Determines the geometry type for features in the aggs layer. In the aggs layer,
each feature represents a geotile_grid cell. If 'grid' each feature is a Polygon
of the cells bounding box. If 'point' each feature is a Point that is the centroid
of the cell.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Query DSL used to filter documents for the search.
** *`runtime_mappings` (Optional, Record<string, { fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request. These fields take
precedence over mapped fields with the same name.
** *`size` (Optional, number)*: Maximum number of features to return in the hits layer. Accepts 0-10000.
If 0, results don’t include the hits layer.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: Sorts features in the hits layer. By default, the API calculates a bounding
box for each feature. It sorts features based on this box’s diagonal length,
from longest to shortest.
** *`track_total_hits` (Optional, boolean | number)*: Number of hits matching the query to count accurately. If `true`, the exact number
of hits is returned at the cost of some performance. If `false`, the response does
not include the total number of hits matching the query.
** *`with_labels` (Optional, boolean)*: If `true`, the hits and aggs layers will contain additional point features representing
suggested label positions for the original features.

[discrete]
=== search_template
Allows to use the Mustache language to pre-render a search definition.

{ref}/search-template.html[Endpoint documentation]
[source,ts]
----
client.searchTemplate({ ... })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices,
and aliases to search. Supports wildcards (*).
** *`explain` (Optional, boolean)*: If `true`, returns detailed information about score calculation as part of each hit.
** *`id` (Optional, string)*: ID of the search template to use. If no source is specified,
this parameter is required.
** *`params` (Optional, Record<string, User-defined value>)*: Key-value pairs used to replace Mustache variables in the template.
The key is the variable name.
The value is the variable value.
** *`profile` (Optional, boolean)*: If `true`, the query execution is profiled.
** *`source` (Optional, string)*: An inline search template. Supports the same parameters as the search API's
request body. Also supports Mustache variables. If no id is specified, this
parameter is required.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`ccs_minimize_roundtrips` (Optional, boolean)*: If `true`, network round-trips are minimized for cross-cluster search requests.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_throttled` (Optional, boolean)*: If `true`, specified concrete, expanded, or aliased indices are not included in the response when throttled.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`scroll` (Optional, string | -1 | 0)*: Specifies how long a consistent view of the index
should be maintained for scrolled search.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: The type of the search operation.
** *`typed_keys` (Optional, boolean)*: If `true`, the response prefixes aggregation and suggester names with their respective types.

[discrete]
=== terms_enum
The terms enum API  can be used to discover terms in the index that begin with the provided string. It is designed for low-latency look-ups used in auto-complete scenarios.

{ref}/search-terms-enum.html[Endpoint documentation]
[source,ts]
----
client.termsEnum({ index, field })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: List of data streams, indices, and index aliases to search. Wildcard (*) expressions are supported.
** *`field` (string)*: The string to match at the start of indexed terms. If not provided, all terms in the field are considered.
** *`size` (Optional, number)*: How many matching terms to return.
** *`timeout` (Optional, string | -1 | 0)*: The maximum length of time to spend collecting results. Defaults to "1s" (one second). If the timeout is exceeded the complete flag set to false in the response and the results may be partial or empty.
** *`case_insensitive` (Optional, boolean)*: When true the provided search string is matched against index terms without case sensitivity.
** *`index_filter` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Allows to filter an index shard if the provided query rewrites to match_none.
** *`string` (Optional, string)*: The string after which terms in the index should be returned. Allows for a form of pagination if the last result from one request is passed as the search_after parameter for a subsequent request.
** *`search_after` (Optional, string)*

[discrete]
=== termvectors
Returns information and statistics about terms in the fields of a particular document.

{ref}/docs-termvectors.html[Endpoint documentation]
[source,ts]
----
client.termvectors({ index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the index that contains the document.
** *`id` (Optional, string)*: Unique identifier of the document.
** *`doc` (Optional, object)*: An artificial document (a document not present in the index) for which you want to retrieve term vectors.
** *`filter` (Optional, { max_doc_freq, max_num_terms, max_term_freq, max_word_length, min_doc_freq, min_term_freq, min_word_length })*: Filter terms based on their tf-idf scores.
** *`per_field_analyzer` (Optional, Record<string, string>)*: Overrides the default per-field analyzer.
** *`fields` (Optional, string | string[])*: List or wildcard expressions of fields to include in the statistics.
Used as the default list unless a specific field list is provided in the `completion_fields` or `fielddata_fields` parameters.
** *`field_statistics` (Optional, boolean)*: If `true`, the response includes the document count, sum of document frequencies, and sum of total term frequencies.
** *`offsets` (Optional, boolean)*: If `true`, the response includes term offsets.
** *`payloads` (Optional, boolean)*: If `true`, the response includes term payloads.
** *`positions` (Optional, boolean)*: If `true`, the response includes term positions.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`realtime` (Optional, boolean)*: If true, the request is real-time as opposed to near-real-time.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`term_statistics` (Optional, boolean)*: If `true`, the response includes term frequency and document frequency.
** *`version` (Optional, number)*: If `true`, returns the document version as part of a hit.
** *`version_type` (Optional, Enum("internal" | "external" | "external_gte" | "force"))*: Specific version type.

[discrete]
=== update
Updates a document with a script or partial document.

{ref}/docs-update.html[Endpoint documentation]
[source,ts]
----
client.update({ id, index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Document ID
** *`index` (string)*: The name of the index
** *`detect_noop` (Optional, boolean)*: Set to false to disable setting 'result' in the response
to 'noop' if no change to the document occurred.
** *`doc` (Optional, object)*: A partial update to an existing document.
** *`doc_as_upsert` (Optional, boolean)*: Set to true to use the contents of 'doc' as the value of 'upsert'
** *`script` (Optional, { lang, options, source } | { id })*: Script to execute to update the document.
** *`scripted_upsert` (Optional, boolean)*: Set to true to execute the script whether or not the document exists.
** *`_source` (Optional, boolean | { excludes, includes })*: Set to false to disable source retrieval. You can also specify a comma-separated
list of the fields you want to retrieve.
** *`upsert` (Optional, object)*: If the document does not already exist, the contents of 'upsert' are inserted as a
new document. If the document exists, the 'script' is executed.
** *`if_primary_term` (Optional, number)*: Only perform the operation if the document has this primary term.
** *`if_seq_no` (Optional, number)*: Only perform the operation if the document has this sequence number.
** *`lang` (Optional, string)*: The script language.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If 'true', Elasticsearch refreshes the affected shards to make this operation
visible to search, if 'wait_for' then wait for a refresh to make this operation
visible to search, if 'false' do nothing with refreshes.
** *`require_alias` (Optional, boolean)*: If true, the destination must be an index alias.
** *`retry_on_conflict` (Optional, number)*: Specify how many times should the operation be retried when a conflict occurs.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for dynamic mapping updates and active shards.
This guarantees Elasticsearch waits for at least the timeout before failing.
The actual wait time could be longer, particularly when multiple waits occur.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operations.
Set to 'all' or any positive integer up to the total number of shards in the index
(number_of_replicas+1). Defaults to 1 meaning the primary shard.
** *`_source_excludes` (Optional, string | string[])*: Specify the source fields you want to exclude.
** *`_source_includes` (Optional, string | string[])*: Specify the source fields you want to retrieve.

[discrete]
=== update_by_query
Performs an update on every document in the index without changing the source,
for example to pick up a mapping change.

{ref}/docs-update-by-query.html[Endpoint documentation]
[source,ts]
----
client.updateByQuery({ index })
----
[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams or indices, omit this parameter or use `*` or `_all`.
** *`max_docs` (Optional, number)*: The maximum number of documents to update.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Specifies the documents to update using the Query DSL.
** *`script` (Optional, { lang, options, source } | { id })*: The script to run to update the document source or metadata when updating.
** *`slice` (Optional, { field, id, max })*: Slice the request manually using the provided slice ID and total number of slices.
** *`conflicts` (Optional, Enum("abort" | "proceed"))*: What to do if update by query hits version conflicts: `abort` or `proceed`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
For example, a request targeting `foo*,bar*` returns an error if an index starts with `foo` but no index starts with `bar`.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`from` (Optional, number)*: Starting offset (default: 0)
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`pipeline` (Optional, string)*: ID of the pipeline to use to preprocess incoming documents.
If the index has a default ingest pipeline specified, then setting the value to `_none` disables the default ingest pipeline for this request.
If a final pipeline is configured it will always run, regardless of the value of this parameter.
** *`preference` (Optional, string)*: Specifies the node or shard the operation should be performed on.
Random by default.
** *`refresh` (Optional, boolean)*: If `true`, Elasticsearch refreshes affected shards to make the operation visible to search.
** *`request_cache` (Optional, boolean)*: If `true`, the request cache is used for this request.
** *`requests_per_second` (Optional, float)*: The throttle for this request in sub-requests per second.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`scroll` (Optional, string | -1 | 0)*: Period to retain the search context for scrolling.
** *`scroll_size` (Optional, number)*: Size of the scroll request that powers the operation.
** *`search_timeout` (Optional, string | -1 | 0)*: Explicit timeout for each search request.
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: The type of the search operation. Available options: `query_then_fetch`, `dfs_query_then_fetch`.
** *`slices` (Optional, number | Enum("auto"))*: The number of slices this task should be divided into.
** *`sort` (Optional, string[])*: A list of <field>:<direction> pairs.
** *`stats` (Optional, string[])*: Specific `tag` of the request for logging and statistical purposes.
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard.
If a query reaches this limit, Elasticsearch terminates the query early.
Elasticsearch collects documents before sorting.
Use with caution.
Elasticsearch applies this parameter to each shard handling the request.
When possible, let Elasticsearch perform early termination automatically.
Avoid specifying this parameter for requests that target data streams with backing indices across multiple data tiers.
** *`timeout` (Optional, string | -1 | 0)*: Period each update request waits for the following operations: dynamic mapping updates, waiting for active shards.
** *`version` (Optional, boolean)*: If `true`, returns the document version as part of a hit.
** *`version_type` (Optional, boolean)*: Should the document increment the version number (internal) on hit or not (reindex)
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the operation is complete.

[discrete]
=== async_search
[discrete]
==== delete
Deletes an async search by ID. If the search is still running, the search request will be cancelled. Otherwise, the saved search results are deleted.

{ref}/async-search.html[Endpoint documentation]
[source,ts]
----
client.asyncSearch.delete({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: A unique identifier for the async search.

[discrete]
==== get
Retrieves the results of a previously submitted async search request given its ID.

{ref}/async-search.html[Endpoint documentation]
[source,ts]
----
client.asyncSearch.get({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: A unique identifier for the async search.
** *`keep_alive` (Optional, string | -1 | 0)*: Specifies how long the async search should be available in the cluster.
When not specified, the `keep_alive` set with the corresponding submit async request will be used.
Otherwise, it is possible to override the value and extend the validity of the request.
When this period expires, the search, if still running, is cancelled.
If the search is completed, its saved results are deleted.
** *`typed_keys` (Optional, boolean)*: Specify whether aggregation and suggester names should be prefixed by their respective types in the response
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Specifies to wait for the search to be completed up until the provided timeout.
Final results will be returned if available before the timeout expires, otherwise the currently available results will be returned once the timeout expires.
By default no timeout is set meaning that the currently available results will be returned without any additional wait.

[discrete]
==== status
Retrieves the status of a previously submitted async search request given its ID.

{ref}/async-search.html[Endpoint documentation]
[source,ts]
----
client.asyncSearch.status({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: A unique identifier for the async search.

[discrete]
==== submit
Executes a search request asynchronously.

{ref}/async-search.html[Endpoint documentation]
[source,ts]
----
client.asyncSearch.submit({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: A list of index names to search; use `_all` or empty string to perform the operation on all indices
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, cardinality, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*
** *`collapse` (Optional, { field, inner_hits, max_concurrent_group_searches, collapse })*
** *`explain` (Optional, boolean)*: If true, returns detailed information about score computation as part of a hit.
** *`ext` (Optional, Record<string, User-defined value>)*: Configuration of search extensions defined by Elasticsearch plugins.
** *`from` (Optional, number)*: Starting document offset. By default, you cannot page through more than 10,000
hits using the from and size parameters. To page through more hits, use the
search_after parameter.
** *`highlight` (Optional, { encoder, fields })*
** *`track_total_hits` (Optional, boolean | number)*: Number of hits matching the query to count accurately. If true, the exact
number of hits is returned at the cost of some performance. If false, the
response does not include the total number of hits matching the query.
Defaults to 10,000 hits.
** *`indices_boost` (Optional, Record<string, number>[])*: Boosts the _score of documents from specified indices.
** *`docvalue_fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (*) patterns. The request returns doc values for field
names matching these patterns in the hits.fields property of the response.
** *`min_score` (Optional, number)*: Minimum _score for matching documents. Documents with a lower _score are
not included in the search results.
** *`post_filter` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*
** *`profile` (Optional, boolean)*
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Defines the search definition using the Query DSL.
** *`rescore` (Optional, { query, window_size } | { query, window_size }[])*
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Retrieve a script evaluation (based on different fields) for each hit.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*
** *`size` (Optional, number)*: The number of hits to return. By default, you cannot page through more
than 10,000 hits using the from and size parameters. To page through more
hits, use the search_after parameter.
** *`slice` (Optional, { field, id, max })*
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*
** *`_source` (Optional, boolean | { excludes, includes })*: Indicates which source fields are returned for matching documents. These
fields are returned in the hits._source property of the search response.
** *`fields` (Optional, { field, format, include_unmapped }[])*: Array of wildcard (*) patterns. The request returns values for field names
matching these patterns in the hits.fields property of the response.
** *`suggest` (Optional, { text })*
** *`terminate_after` (Optional, number)*: Maximum number of documents to collect for each shard. If a query reaches this
limit, Elasticsearch terminates the query early. Elasticsearch collects documents
before sorting. Defaults to 0, which does not terminate query execution early.
** *`timeout` (Optional, string)*: Specifies the period of time to wait for a response from each shard. If no response
is received before the timeout expires, the request fails and returns an error.
Defaults to no timeout.
** *`track_scores` (Optional, boolean)*: If true, calculate and return document scores, even if the scores are not used for sorting.
** *`version` (Optional, boolean)*: If true, returns document version as part of a hit.
** *`seq_no_primary_term` (Optional, boolean)*: If true, returns sequence number and primary term of the last modification
of each hit. See Optimistic concurrency control.
** *`stored_fields` (Optional, string | string[])*: List of stored fields to return as part of a hit. If no fields are specified,
no stored fields are included in the response. If this field is specified, the _source
parameter defaults to false. You can pass _source: true to return both source fields
and stored fields in the search response.
** *`pit` (Optional, { id, keep_alive })*: Limits the search to a point in time (PIT). If you provide a PIT, you
cannot specify an <index> in the request path.
** *`runtime_mappings` (Optional, Record<string, { fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request. These fields take
precedence over mapped fields with the same name.
** *`stats` (Optional, string[])*: Stats groups to associate with the search. Each group maintains a statistics
aggregation for its associated searches. You can retrieve these stats using
the indices stats API.
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Blocks and waits until the search is completed up to a certain timeout.
When the async search completes within the timeout, the response won’t include the ID as the results are not stored in the cluster.
** *`keep_on_completion` (Optional, boolean)*: If `true`, results are stored for later retrieval when the search completes within the `wait_for_completion_timeout`.
** *`keep_alive` (Optional, string | -1 | 0)*: Specifies how long the async search needs to be available.
Ongoing async searches and any saved search results are deleted after this period.
** *`allow_no_indices` (Optional, boolean)*: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes `_all` string or when no indices have been specified)
** *`allow_partial_search_results` (Optional, boolean)*: Indicate if an error should be returned if there is a partial search failure or timeout
** *`analyzer` (Optional, string)*: The analyzer to use for the query string
** *`analyze_wildcard` (Optional, boolean)*: Specify whether wildcard and prefix queries should be analyzed (default: false)
** *`batched_reduce_size` (Optional, number)*: Affects how often partial results become available, which happens whenever shard results are reduced.
A partial reduction is performed every time the coordinating node has received a certain number of new shard responses (5 by default).
** *`ccs_minimize_roundtrips` (Optional, boolean)*: The default value is the only supported value.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query (AND or OR)
** *`df` (Optional, string)*: The field to use as default where no field prefix is given in the query string
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`ignore_throttled` (Optional, boolean)*: Whether specified concrete, expanded or aliased indices should be ignored when throttled
** *`ignore_unavailable` (Optional, boolean)*: Whether specified concrete indices should be ignored when unavailable (missing or closed)
** *`lenient` (Optional, boolean)*: Specify whether format-based query failures (such as providing text to a numeric field) should be ignored
** *`max_concurrent_shard_requests` (Optional, number)*: The number of concurrent shard requests per node this search executes concurrently. This value should be used to limit the impact of the search on the cluster in order to limit the number of concurrent shard requests
** *`min_compatible_shard_node` (Optional, string)*
** *`preference` (Optional, string)*: Specify the node or shard the operation should be performed on (default: random)
** *`pre_filter_shard_size` (Optional, number)*: The default value cannot be changed, which enforces the execution of a pre-filter roundtrip to retrieve statistics from each shard so that the ones that surely don’t hold any document matching the query get skipped.
** *`request_cache` (Optional, boolean)*: Specify if request cache should be used for this request or not, defaults to true
** *`routing` (Optional, string)*: A list of specific routing values
** *`scroll` (Optional, string | -1 | 0)*
** *`search_type` (Optional, Enum("query_then_fetch" | "dfs_query_then_fetch"))*: Search operation type
** *`suggest_field` (Optional, string)*: Specifies which field to use for suggestions.
** *`suggest_mode` (Optional, Enum("missing" | "popular" | "always"))*: Specify suggest mode
** *`suggest_size` (Optional, number)*: How many suggestions to return in response
** *`suggest_text` (Optional, string)*: The source text for which the suggestions should be returned.
** *`typed_keys` (Optional, boolean)*: Specify whether aggregation and suggester names should be prefixed by their respective types in the response
** *`rest_total_hits_as_int` (Optional, boolean)*
** *`_source_excludes` (Optional, string | string[])*: A list of fields to exclude from the returned _source field
** *`_source_includes` (Optional, string | string[])*: A list of fields to extract and return from the _source field
** *`q` (Optional, string)*: Query in the Lucene query string syntax

[discrete]
=== cat
[discrete]
==== aliases
Shows information about currently configured aliases to indices including filter and routing infos.

{ref}/cat-alias.html[Endpoint documentation]
[source,ts]
----
client.cat.aliases({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: A list of aliases to retrieve. Supports wildcards (`*`).  To retrieve all aliases, omit this parameter or use `*` or `_all`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.

[discrete]
==== component_templates
Returns information about existing component_templates templates.

{ref}/cat-component-templates.html[Endpoint documentation]
[source,ts]
----
client.cat.componentTemplates({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: The name of the component template. Accepts wildcard expressions. If omitted, all component templates are returned.

[discrete]
==== count
Provides quick access to the document count of the entire cluster, or individual indices.

{ref}/cat-count.html[Endpoint documentation]
[source,ts]
----
client.cat.count({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`). To target all data streams and indices, omit this parameter or use `*` or `_all`.

[discrete]
==== help
Returns help for the Cat APIs.

{ref}/cat.html[Endpoint documentation]
[source,ts]
----
client.cat.help()
----


[discrete]
==== indices
Returns information about indices: number of primaries and replicas, document counts, disk size, ...

{ref}/cat-indices.html[Endpoint documentation]
[source,ts]
----
client.cat.indices({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`). To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: The type of index that wildcard patterns can match.
** *`health` (Optional, Enum("green" | "yellow" | "red"))*: The health status used to limit returned indices. By default, the response includes indices of any health status.
** *`include_unloaded_segments` (Optional, boolean)*: If true, the response includes information from segments that are not loaded into memory.
** *`pri` (Optional, boolean)*: If true, the response only includes information from primary shards.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.

[discrete]
==== ml_data_frame_analytics
Gets configuration and usage information about data frame analytics jobs.

{ref}/cat-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.cat.mlDataFrameAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: The ID of the data frame analytics to fetch
** *`allow_no_match` (Optional, boolean)*: Whether to ignore if a wildcard expression matches no configs. (This includes `_all` string or when no configs have been specified)
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit in which to display byte values
** *`h` (Optional, Enum("assignment_explanation" | "create_time" | "description" | "dest_index" | "failure_reason" | "id" | "model_memory_limit" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "progress" | "source_index" | "state" | "type" | "version") | Enum("assignment_explanation" | "create_time" | "description" | "dest_index" | "failure_reason" | "id" | "model_memory_limit" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "progress" | "source_index" | "state" | "type" | "version")[])*: List of column names to display.
** *`s` (Optional, Enum("assignment_explanation" | "create_time" | "description" | "dest_index" | "failure_reason" | "id" | "model_memory_limit" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "progress" | "source_index" | "state" | "type" | "version") | Enum("assignment_explanation" | "create_time" | "description" | "dest_index" | "failure_reason" | "id" | "model_memory_limit" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "progress" | "source_index" | "state" | "type" | "version")[])*: List of column names or column aliases used to sort the
response.
** *`time` (Optional, string | -1 | 0)*: Unit used to display time values.

[discrete]
==== ml_datafeeds
Gets configuration and usage information about datafeeds.

{ref}/cat-datafeeds.html[Endpoint documentation]
[source,ts]
----
client.cat.mlDatafeeds({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (Optional, string)*: A numerical character string that uniquely identifies the datafeed.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

* Contains wildcard expressions and there are no datafeeds that match.
* Contains the `_all` string or no identifiers and there are no matches.
* Contains wildcard expressions and there are only partial matches.

If `true`, the API returns an empty datafeeds array when there are no matches and the subset of results when
there are partial matches. If `false`, the API returns a 404 status code when there are no matches or only
partial matches.
** *`h` (Optional, Enum("ae" | "bc" | "id" | "na" | "ne" | "ni" | "nn" | "sba" | "sc" | "seah" | "st" | "s") | Enum("ae" | "bc" | "id" | "na" | "ne" | "ni" | "nn" | "sba" | "sc" | "seah" | "st" | "s")[])*: List of column names to display.
** *`s` (Optional, Enum("ae" | "bc" | "id" | "na" | "ne" | "ni" | "nn" | "sba" | "sc" | "seah" | "st" | "s") | Enum("ae" | "bc" | "id" | "na" | "ne" | "ni" | "nn" | "sba" | "sc" | "seah" | "st" | "s")[])*: List of column names or column aliases used to sort the response.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.

[discrete]
==== ml_jobs
Gets configuration and usage information about anomaly detection jobs.

{ref}/cat-anomaly-detectors.html[Endpoint documentation]
[source,ts]
----
client.cat.mlJobs({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (Optional, string)*: Identifier for the anomaly detection job.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

* Contains wildcard expressions and there are no jobs that match.
* Contains the `_all` string or no identifiers and there are no matches.
* Contains wildcard expressions and there are only partial matches.

If `true`, the API returns an empty jobs array when there are no matches and the subset of results when there
are partial matches. If `false`, the API returns a 404 status code when there are no matches or only partial
matches.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.
** *`h` (Optional, Enum("assignment_explanation" | "buckets.count" | "buckets.time.exp_avg" | "buckets.time.exp_avg_hour" | "buckets.time.max" | "buckets.time.min" | "buckets.time.total" | "data.buckets" | "data.earliest_record" | "data.empty_buckets" | "data.input_bytes" | "data.input_fields" | "data.input_records" | "data.invalid_dates" | "data.last" | "data.last_empty_bucket" | "data.last_sparse_bucket" | "data.latest_record" | "data.missing_fields" | "data.out_of_order_timestamps" | "data.processed_fields" | "data.processed_records" | "data.sparse_buckets" | "forecasts.memory.avg" | "forecasts.memory.max" | "forecasts.memory.min" | "forecasts.memory.total" | "forecasts.records.avg" | "forecasts.records.max" | "forecasts.records.min" | "forecasts.records.total" | "forecasts.time.avg" | "forecasts.time.max" | "forecasts.time.min" | "forecasts.time.total" | "forecasts.total" | "id" | "model.bucket_allocation_failures" | "model.by_fields" | "model.bytes" | "model.bytes_exceeded" | "model.categorization_status" | "model.categorized_doc_count" | "model.dead_category_count" | "model.failed_category_count" | "model.frequent_category_count" | "model.log_time" | "model.memory_limit" | "model.memory_status" | "model.over_fields" | "model.partition_fields" | "model.rare_category_count" | "model.timestamp" | "model.total_category_count" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "opened_time" | "state") | Enum("assignment_explanation" | "buckets.count" | "buckets.time.exp_avg" | "buckets.time.exp_avg_hour" | "buckets.time.max" | "buckets.time.min" | "buckets.time.total" | "data.buckets" | "data.earliest_record" | "data.empty_buckets" | "data.input_bytes" | "data.input_fields" | "data.input_records" | "data.invalid_dates" | "data.last" | "data.last_empty_bucket" | "data.last_sparse_bucket" | "data.latest_record" | "data.missing_fields" | "data.out_of_order_timestamps" | "data.processed_fields" | "data.processed_records" | "data.sparse_buckets" | "forecasts.memory.avg" | "forecasts.memory.max" | "forecasts.memory.min" | "forecasts.memory.total" | "forecasts.records.avg" | "forecasts.records.max" | "forecasts.records.min" | "forecasts.records.total" | "forecasts.time.avg" | "forecasts.time.max" | "forecasts.time.min" | "forecasts.time.total" | "forecasts.total" | "id" | "model.bucket_allocation_failures" | "model.by_fields" | "model.bytes" | "model.bytes_exceeded" | "model.categorization_status" | "model.categorized_doc_count" | "model.dead_category_count" | "model.failed_category_count" | "model.frequent_category_count" | "model.log_time" | "model.memory_limit" | "model.memory_status" | "model.over_fields" | "model.partition_fields" | "model.rare_category_count" | "model.timestamp" | "model.total_category_count" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "opened_time" | "state")[])*: List of column names to display.
** *`s` (Optional, Enum("assignment_explanation" | "buckets.count" | "buckets.time.exp_avg" | "buckets.time.exp_avg_hour" | "buckets.time.max" | "buckets.time.min" | "buckets.time.total" | "data.buckets" | "data.earliest_record" | "data.empty_buckets" | "data.input_bytes" | "data.input_fields" | "data.input_records" | "data.invalid_dates" | "data.last" | "data.last_empty_bucket" | "data.last_sparse_bucket" | "data.latest_record" | "data.missing_fields" | "data.out_of_order_timestamps" | "data.processed_fields" | "data.processed_records" | "data.sparse_buckets" | "forecasts.memory.avg" | "forecasts.memory.max" | "forecasts.memory.min" | "forecasts.memory.total" | "forecasts.records.avg" | "forecasts.records.max" | "forecasts.records.min" | "forecasts.records.total" | "forecasts.time.avg" | "forecasts.time.max" | "forecasts.time.min" | "forecasts.time.total" | "forecasts.total" | "id" | "model.bucket_allocation_failures" | "model.by_fields" | "model.bytes" | "model.bytes_exceeded" | "model.categorization_status" | "model.categorized_doc_count" | "model.dead_category_count" | "model.failed_category_count" | "model.frequent_category_count" | "model.log_time" | "model.memory_limit" | "model.memory_status" | "model.over_fields" | "model.partition_fields" | "model.rare_category_count" | "model.timestamp" | "model.total_category_count" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "opened_time" | "state") | Enum("assignment_explanation" | "buckets.count" | "buckets.time.exp_avg" | "buckets.time.exp_avg_hour" | "buckets.time.max" | "buckets.time.min" | "buckets.time.total" | "data.buckets" | "data.earliest_record" | "data.empty_buckets" | "data.input_bytes" | "data.input_fields" | "data.input_records" | "data.invalid_dates" | "data.last" | "data.last_empty_bucket" | "data.last_sparse_bucket" | "data.latest_record" | "data.missing_fields" | "data.out_of_order_timestamps" | "data.processed_fields" | "data.processed_records" | "data.sparse_buckets" | "forecasts.memory.avg" | "forecasts.memory.max" | "forecasts.memory.min" | "forecasts.memory.total" | "forecasts.records.avg" | "forecasts.records.max" | "forecasts.records.min" | "forecasts.records.total" | "forecasts.time.avg" | "forecasts.time.max" | "forecasts.time.min" | "forecasts.time.total" | "forecasts.total" | "id" | "model.bucket_allocation_failures" | "model.by_fields" | "model.bytes" | "model.bytes_exceeded" | "model.categorization_status" | "model.categorized_doc_count" | "model.dead_category_count" | "model.failed_category_count" | "model.frequent_category_count" | "model.log_time" | "model.memory_limit" | "model.memory_status" | "model.over_fields" | "model.partition_fields" | "model.rare_category_count" | "model.timestamp" | "model.total_category_count" | "node.address" | "node.ephemeral_id" | "node.id" | "node.name" | "opened_time" | "state")[])*: List of column names or column aliases used to sort the response.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.

[discrete]
==== ml_trained_models
Gets configuration and usage information about inference trained models.

{ref}/cat-trained-model.html[Endpoint documentation]
[source,ts]
----
client.cat.mlTrainedModels({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (Optional, string)*: A unique identifier for the trained model.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request: contains wildcard expressions and there are no models that match; contains the `_all` string or no identifiers and there are no matches; contains wildcard expressions and there are only partial matches.
If `true`, the API returns an empty array when there are no matches and the subset of results when there are partial matches.
If `false`, the API returns a 404 status code when there are no matches or only partial matches.
** *`bytes` (Optional, Enum("b" | "kb" | "mb" | "gb" | "tb" | "pb"))*: The unit used to display byte values.
** *`h` (Optional, Enum("create_time" | "created_by" | "data_frame_analytics_id" | "description" | "heap_size" | "id" | "ingest.count" | "ingest.current" | "ingest.failed" | "ingest.pipelines" | "ingest.time" | "license" | "operations" | "version") | Enum("create_time" | "created_by" | "data_frame_analytics_id" | "description" | "heap_size" | "id" | "ingest.count" | "ingest.current" | "ingest.failed" | "ingest.pipelines" | "ingest.time" | "license" | "operations" | "version")[])*: A list of column names to display.
** *`s` (Optional, Enum("create_time" | "created_by" | "data_frame_analytics_id" | "description" | "heap_size" | "id" | "ingest.count" | "ingest.current" | "ingest.failed" | "ingest.pipelines" | "ingest.time" | "license" | "operations" | "version") | Enum("create_time" | "created_by" | "data_frame_analytics_id" | "description" | "heap_size" | "id" | "ingest.count" | "ingest.current" | "ingest.failed" | "ingest.pipelines" | "ingest.time" | "license" | "operations" | "version")[])*: A list of column names or aliases used to sort the response.
** *`from` (Optional, number)*: Skips the specified number of transforms.
** *`size` (Optional, number)*: The maximum number of transforms to display.

[discrete]
==== transforms
Gets configuration and usage information about transforms.

{ref}/cat-transforms.html[Endpoint documentation]
[source,ts]
----
client.cat.transforms({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (Optional, string)*: A transform identifier or a wildcard expression.
If you do not specify one of these options, the API returns information for all transforms.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request: contains wildcard expressions and there are no transforms that match; contains the `_all` string or no identifiers and there are no matches; contains wildcard expressions and there are only partial matches.
If `true`, it returns an empty transforms array when there are no matches and the subset of results when there are partial matches.
If `false`, the request returns a 404 status code when there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of transforms.
** *`h` (Optional, Enum("changes_last_detection_time" | "checkpoint" | "checkpoint_duration_time_exp_avg" | "checkpoint_progress" | "create_time" | "delete_time" | "description" | "dest_index" | "documents_deleted" | "documents_indexed" | "docs_per_second" | "documents_processed" | "frequency" | "id" | "index_failure" | "index_time" | "index_total" | "indexed_documents_exp_avg" | "last_search_time" | "max_page_search_size" | "pages_processed" | "pipeline" | "processed_documents_exp_avg" | "processing_time" | "reason" | "search_failure" | "search_time" | "search_total" | "source_index" | "state" | "transform_type" | "trigger_count" | "version") | Enum("changes_last_detection_time" | "checkpoint" | "checkpoint_duration_time_exp_avg" | "checkpoint_progress" | "create_time" | "delete_time" | "description" | "dest_index" | "documents_deleted" | "documents_indexed" | "docs_per_second" | "documents_processed" | "frequency" | "id" | "index_failure" | "index_time" | "index_total" | "indexed_documents_exp_avg" | "last_search_time" | "max_page_search_size" | "pages_processed" | "pipeline" | "processed_documents_exp_avg" | "processing_time" | "reason" | "search_failure" | "search_time" | "search_total" | "source_index" | "state" | "transform_type" | "trigger_count" | "version")[])*: List of column names to display.
** *`s` (Optional, Enum("changes_last_detection_time" | "checkpoint" | "checkpoint_duration_time_exp_avg" | "checkpoint_progress" | "create_time" | "delete_time" | "description" | "dest_index" | "documents_deleted" | "documents_indexed" | "docs_per_second" | "documents_processed" | "frequency" | "id" | "index_failure" | "index_time" | "index_total" | "indexed_documents_exp_avg" | "last_search_time" | "max_page_search_size" | "pages_processed" | "pipeline" | "processed_documents_exp_avg" | "processing_time" | "reason" | "search_failure" | "search_time" | "search_total" | "source_index" | "state" | "transform_type" | "trigger_count" | "version") | Enum("changes_last_detection_time" | "checkpoint" | "checkpoint_duration_time_exp_avg" | "checkpoint_progress" | "create_time" | "delete_time" | "description" | "dest_index" | "documents_deleted" | "documents_indexed" | "docs_per_second" | "documents_processed" | "frequency" | "id" | "index_failure" | "index_time" | "index_total" | "indexed_documents_exp_avg" | "last_search_time" | "max_page_search_size" | "pages_processed" | "pipeline" | "processed_documents_exp_avg" | "processing_time" | "reason" | "search_failure" | "search_time" | "search_total" | "source_index" | "state" | "transform_type" | "trigger_count" | "version")[])*: List of column names or column aliases used to sort the response.
** *`time` (Optional, Enum("nanos" | "micros" | "ms" | "s" | "m" | "h" | "d"))*: The unit used to display time values.
** *`size` (Optional, number)*: The maximum number of transforms to obtain.

[discrete]
=== cluster
[discrete]
==== delete_component_template
Deletes a component template

{ref}/indices-component-template.html[Endpoint documentation]
[source,ts]
----
client.cluster.deleteComponentTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List or wildcard expression of component template names used to limit the request.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== exists_component_template
Returns information about whether a particular component template exist

{ref}/indices-component-template.html[Endpoint documentation]
[source,ts]
----
client.cluster.existsComponentTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of component template names used to limit the request.
Wildcard (*) expressions are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an
error.
** *`local` (Optional, boolean)*: If true, the request retrieves information from the local node only.
Defaults to false, which means information is retrieved from the master node.

[discrete]
==== get_component_template
Returns one or more component templates

{ref}/indices-component-template.html[Endpoint documentation]
[source,ts]
----
client.cluster.getComponentTemplate({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: List of component template names used to limit the request.
Wildcard (`*`) expressions are supported.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.
If `false`, information is retrieved from the master node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== info
Returns different information about the cluster.

{ref}/cluster-info.html[Endpoint documentation]
[source,ts]
----
client.cluster.info({ target })
----

[discrete]
==== Arguments

* *Request (object):*
** *`target` (Enum("_all" | "http" | "ingest" | "thread_pool" | "script") | Enum("_all" | "http" | "ingest" | "thread_pool" | "script")[])*: Limits the information returned to the specific target. Supports a list, such as http,ingest.

[discrete]
==== put_component_template
Creates or updates a component template

{ref}/indices-component-template.html[Endpoint documentation]
[source,ts]
----
client.cluster.putComponentTemplate({ name, template })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the component template to create.
Elasticsearch includes the following built-in component templates: `logs-mappings`; 'logs-settings`; `metrics-mappings`; `metrics-settings`;`synthetics-mapping`; `synthetics-settings`.
Elastic Agent uses these templates to configure backing indices for its data streams.
If you use Elastic Agent and want to overwrite one of these templates, set the `version` for your replacement template higher than the current version.
If you don’t use Elastic Agent and want to disable all built-in component and index templates, set `stack.templates.enabled` to `false` using the cluster update settings API.
** *`template` ({ aliases, mappings, settings, defaults, data_stream })*: The template to be applied which includes mappings, settings, or aliases configuration.
** *`allow_auto_create` (Optional, boolean)*: This setting overrides the value of the `action.auto_create_index` cluster setting.
If set to `true` in a template, then indices can be automatically created using that
template even if auto-creation of indices is disabled via `actions.auto_create_index`.
If set to `false` then data streams matching the template must always be explicitly created.
** *`version` (Optional, number)*: Version number used to manage component templates externally.
This number isn't automatically generated or incremented by Elasticsearch.
To unset a version, replace the template without specifying a version.
** *`_meta` (Optional, Record<string, User-defined value>)*: Optional user metadata about the component template.
May have any contents. This map is not automatically generated by Elasticsearch.
This information is stored in the cluster state, so keeping it short is preferable.
To unset `_meta`, replace the template without specifying this information.
** *`create` (Optional, boolean)*: If `true`, this request cannot replace or update existing component templates.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
=== enrich
[discrete]
==== delete_policy
Deletes an existing enrich policy and its enrich index.

{ref}/delete-enrich-policy-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.deletePolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Enrich policy to delete.

[discrete]
==== execute_policy
Creates the enrich index for an existing enrich policy.

{ref}/execute-enrich-policy-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.executePolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Enrich policy to execute.
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks other enrich policy execution requests until complete.

[discrete]
==== get_policy
Gets information about an enrich policy.

{ref}/get-enrich-policy-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.getPolicy({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: List of enrich policy names used to limit the request.
To return information for all enrich policies, omit this parameter.

[discrete]
==== put_policy
Creates a new enrich policy.

{ref}/put-enrich-policy-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.putPolicy({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the enrich policy to create or update.
** *`geo_match` (Optional, { enrich_fields, indices, match_field, query, name, elasticsearch_version })*: Matches enrich data to incoming documents based on a `geo_shape` query.
** *`match` (Optional, { enrich_fields, indices, match_field, query, name, elasticsearch_version })*: Matches enrich data to incoming documents based on a `term` query.
** *`range` (Optional, { enrich_fields, indices, match_field, query, name, elasticsearch_version })*: Matches a number, date, or IP address in incoming documents to a range in the enrich index based on a `term` query.

[discrete]
==== stats
Gets enrich coordinator statistics and information about enrich policies that are currently executing.

{ref}/enrich-stats-api.html[Endpoint documentation]
[source,ts]
----
client.enrich.stats()
----


[discrete]
=== eql
[discrete]
==== delete
Deletes an async EQL search by ID. If the search is still running, the search request will be cancelled. Otherwise, the saved search results are deleted.

{ref}/eql-search-api.html[Endpoint documentation]
[source,ts]
----
client.eql.delete({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search to delete.
A search ID is provided in the EQL search API's response for an async search.
A search ID is also provided if the request’s `keep_on_completion` parameter is `true`.

[discrete]
==== get
Returns async results from previously executed Event Query Language (EQL) search

{ref}/get-async-eql-search-api.html[Endpoint documentation]
[source,ts]
----
client.eql.get({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.
** *`keep_alive` (Optional, string | -1 | 0)*: Period for which the search and its results are stored on the cluster.
Defaults to the keep_alive value set by the search’s EQL search API request.
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Timeout duration to wait for the request to finish.
Defaults to no timeout, meaning the request waits for complete search results.

[discrete]
==== get_status
Returns the status of a previously submitted async or stored Event Query Language (EQL) search

{ref}/get-async-eql-status-api.html[Endpoint documentation]
[source,ts]
----
client.eql.getStatus({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.

[discrete]
==== search
Returns results matching a query expressed in Event Query Language (EQL)

{ref}/eql-search-api.html[Endpoint documentation]
[source,ts]
----
client.eql.search({ index, query })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: The name of the index to scope the operation
** *`query` (string)*: EQL query you wish to run.
** *`case_sensitive` (Optional, boolean)*
** *`event_category_field` (Optional, string)*: Field containing the event classification, such as process, file, or network.
** *`tiebreaker_field` (Optional, string)*: Field used to sort hits with the same timestamp in ascending order
** *`timestamp_field` (Optional, string)*: Field containing event timestamp. Default "@timestamp"
** *`fetch_size` (Optional, number)*: Maximum number of events to search at a time for sequence queries.
** *`filter` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type } | { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type }[])*: Query, written in Query DSL, used to filter the events on which the EQL query runs.
** *`keep_alive` (Optional, string | -1 | 0)*
** *`keep_on_completion` (Optional, boolean)*
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*
** *`size` (Optional, number)*: For basic queries, the maximum number of matching events to return. Defaults to 10
** *`fields` (Optional, { field, format, include_unmapped } | { field, format, include_unmapped }[])*: Array of wildcard (*) patterns. The response returns values for field names matching these patterns in the fields property of each hit.
** *`result_position` (Optional, Enum("tail" | "head"))*
** *`allow_no_indices` (Optional, boolean)*
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*
** *`ignore_unavailable` (Optional, boolean)*: If true, missing or closed indices are not included in the response.

[discrete]
=== graph
[discrete]
==== explore
Explore extracted and summarized information about the documents and terms in an index.

{ref}/graph-explore-api.html[Endpoint documentation]
[source,ts]
----
client.graph.explore({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: Name of the index.
** *`connections` (Optional, { connections, query, vertices })*: Specifies or more fields from which you want to extract terms that are associated with the specified vertices.
** *`controls` (Optional, { sample_diversity, sample_size, timeout, use_significance })*: Direct the Graph API how to build the graph.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: A seed query that identifies the documents of interest. Can be any valid Elasticsearch query.
** *`vertices` (Optional, { exclude, field, include, min_doc_count, shard_min_doc_count, size }[])*: Specifies one or more fields that contain the terms you want to include in the graph as vertices.
** *`routing` (Optional, string)*: Custom value used to route operations to a specific shard.
** *`timeout` (Optional, string | -1 | 0)*: Specifies the period of time to wait for a response from each shard.
If no response is received before the timeout expires, the request fails and returns an error.
Defaults to no timeout.

[discrete]
=== indices
[discrete]
==== add_block
Adds a block to an index.

{ref}/index-modules-blocks.html[Endpoint documentation]
[source,ts]
----
client.indices.addBlock({ index, block })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: A comma separated list of indices to add a block to
** *`block` (Enum("metadata" | "read" | "read_only" | "write"))*: The block to add (one of read, write, read_only or metadata)
** *`allow_no_indices` (Optional, boolean)*: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes `_all` string or when no indices have been specified)
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether to expand wildcard expression to concrete indices that are open, closed or both.
** *`ignore_unavailable` (Optional, boolean)*: Whether specified concrete indices should be ignored when unavailable (missing or closed)
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master
** *`timeout` (Optional, string | -1 | 0)*: Explicit operation timeout

[discrete]
==== analyze
Performs the analysis process on a text and return the tokens breakdown of the text.

{ref}/indices-analyze.html[Endpoint documentation]
[source,ts]
----
client.indices.analyze({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string)*: Index used to derive the analyzer.
If specified, the `analyzer` or field parameter overrides this value.
If no index is specified or the index does not have a default analyzer, the analyze API uses the standard analyzer.
** *`analyzer` (Optional, string)*: The name of the analyzer that should be applied to the provided `text`.
This could be a built-in analyzer, or an analyzer that’s been configured in the index.
** *`attributes` (Optional, string[])*: Array of token attributes used to filter the output of the `explain` parameter.
** *`char_filter` (Optional, string | { type } | { type, mappings, mappings_path } | { type, flags, pattern, replacement } | { type, mode, name } | { type, normalize_kana, normalize_kanji }[])*: Array of character filters used to preprocess characters before the tokenizer.
** *`explain` (Optional, boolean)*: If `true`, the response includes token attributes and additional details.
** *`field` (Optional, string)*: Field used to derive the analyzer.
To use this parameter, you must specify an index.
If specified, the `analyzer` parameter overrides this value.
** *`filter` (Optional, string | { type, preserve_original } | { type, common_words, common_words_path, ignore_case, query_mode } | { type, filter, script } | { type, delimiter, encoding } | { type, max_gram, min_gram, side, preserve_original } | { type, articles, articles_path, articles_case } | { type, max_output_size, separator } | { type, dedup, dictionary, locale, longest_only } | { type } | { type, mode, types } | { type, keep_words, keep_words_case, keep_words_path } | { type, ignore_case, keywords, keywords_path, keywords_pattern } | { type } | { type, max, min } | { type, consume_all_tokens, max_token_count } | { type, language } | { type, filters, preserve_original } | { type, max_gram, min_gram, preserve_original } | { type, stoptags } | { type, patterns, preserve_original } | { type, all, flags, pattern, replacement } | { type } | { type, script } | { type } | { type } | { type, filler_token, max_shingle_size, min_shingle_size, output_unigrams, output_unigrams_if_no_shingles, token_separator } | { type, language } | { type, rules, rules_path } | { type, language } | { type, ignore_case, remove_trailing, stopwords, stopwords_path } | { type, expand, format, lenient, synonyms, synonyms_path, tokenizer, updateable } | { type, expand, format, lenient, synonyms, synonyms_path, tokenizer, updateable } | { type } | { type, length } | { type, only_on_same_position } | { type } | { type, adjust_offsets, catenate_all, catenate_numbers, catenate_words, generate_number_parts, generate_word_parts, ignore_keywords, preserve_original, protected_words, protected_words_path, split_on_case_change, split_on_numerics, stem_english_possessive, type_table, type_table_path } | { type, catenate_all, catenate_numbers, catenate_words, generate_number_parts, generate_word_parts, preserve_original, protected_words, protected_words_path, split_on_case_change, split_on_numerics, stem_english_possessive, type_table, type_table_path } | { type, minimum_length } | { type, use_romaji } | { type, stoptags } | { type, rule_files } | { type, alternate, caseFirst, caseLevel, country, decomposition, hiraganaQuaternaryMode, language, numeric, rules, strength, variableTop, variant } | { type, unicode_set_filter } | { type, name } | { type, dir, id } | { type, encoder, languageset, max_code_len, name_type, replace, rule_type } | { type }[])*: Array of token filters used to apply after the tokenizer.
** *`normalizer` (Optional, string)*: Normalizer to use to convert text into a single token.
** *`text` (Optional, string | string[])*: Text to analyze.
If an array of strings is provided, it is analyzed as a multi-value field.
** *`tokenizer` (Optional, string | { type, tokenize_on_chars, max_token_length } | { type, custom_token_chars, max_gram, min_gram, token_chars } | { type, buffer_size } | { type } | { type } | { type, custom_token_chars, max_gram, min_gram, token_chars } | { type, decompound_mode, discard_punctuation, user_dictionary, user_dictionary_rules } | { type, buffer_size, delimiter, replacement, reverse, skip } | { type, max_token_length } | { type, max_token_length } | { type, max_token_length } | { type, discard_punctuation, mode, nbest_cost, nbest_examples, user_dictionary, user_dictionary_rules, discard_compound_token } | { type, flags, group, pattern } | { type, rule_files })*: Tokenizer to use to convert text into tokens.

[discrete]
==== create
Creates an index with optional settings and mappings.

{ref}/indices-create-index.html[Endpoint documentation]
[source,ts]
----
client.indices.create({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string)*: Name of the index you wish to create.
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: Aliases for the index.
** *`mappings` (Optional, { all_field, date_detection, dynamic, dynamic_date_formats, dynamic_templates, _field_names, index_field, _meta, numeric_detection, properties, _routing, _size, _source, runtime, enabled })*: Mapping for fields in the index. If specified, this mapping can include:
- Field names
- Field data types
- Mapping parameters
** *`settings` (Optional, { index, mode, routing_path, soft_deletes, sort, number_of_shards, number_of_replicas, number_of_routing_shards, check_on_startup, codec, routing_partition_size, load_fixed_bitset_filters_eagerly, hidden, auto_expand_replicas, merge, search, refresh_interval, max_result_window, max_inner_result_window, max_rescore_window, max_docvalue_fields_search, max_script_fields, max_ngram_diff, max_shingle_diff, blocks, max_refresh_listeners, analyze, highlight, max_terms_count, max_regex_length, routing, gc_deletes, default_pipeline, final_pipeline, lifecycle, provided_name, creation_date, creation_date_string, uuid, version, verified_before_close, format, max_slices_per_scroll, translog, query_string, priority, top_metrics_max_size, analysis, settings, time_series, shards, queries, similarity, mapping, indexing.slowlog, indexing_pressure, store })*: Configuration options for the index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation. 
Set to `all` or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== create_data_stream
Creates a data stream

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.createDataStream({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the data stream, which must meet the following criteria:
Lowercase only;
Cannot include `\`, `/`, `*`, `?`, `"`, `<`, `>`, `|`, `,`, `#`, `:`, or a space character;
Cannot start with `-`, `_`, `+`, or `.ds-`;
Cannot be `.` or `..`;
Cannot be longer than 255 bytes. Multi-byte characters count towards this limit faster.

[discrete]
==== data_streams_stats
Provides statistics on operations happening in a data stream.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.dataStreamsStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: List of data streams used to limit the request.
Wildcard expressions (`*`) are supported.
To target all data streams in a cluster, omit this parameter or use `*`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match.
Supports a list of values, such as `open,hidden`.

[discrete]
==== delete
Deletes an index.

{ref}/indices-delete-index.html[Endpoint documentation]
[source,ts]
----
client.indices.delete({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of indices to delete.
You cannot specify index aliases.
By default, this parameter does not support wildcards (`*`) or `_all`.
To use wildcards or `_all`, set the `action.destructive_requires_name` cluster setting to `false`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== delete_alias
Deletes an alias.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteAlias({ index, name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams or indices used to limit the request.
Supports wildcards (`*`).
** *`name` (string | string[])*: List of aliases to remove.
Supports wildcards (`*`). To remove all aliases, use `*` or `_all`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== delete_data_lifecycle
Deletes the data stream lifecycle of the selected data streams.

{ref}/data-streams-delete-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteDataLifecycle({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: A list of data streams of which the data stream lifecycle will be deleted; use `*` to get all data streams
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Whether wildcard expressions should get expanded to open or closed indices (default: open)
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master
** *`timeout` (Optional, string | -1 | 0)*: Explicit timestamp for the document

[discrete]
==== delete_data_stream
Deletes a data stream.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteDataStream({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of data streams to delete. Wildcard (`*`) expressions are supported.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match. Supports a list of values,such as `open,hidden`.

[discrete]
==== delete_index_template
Deletes an index template.

{ref}/indices-templates.html[Endpoint documentation]
[source,ts]
----
client.indices.deleteIndexTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of index template names used to limit the request. Wildcard (*) expressions are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== exists
Returns information about whether a particular index exists.

{ref}/indices-exists.html[Endpoint documentation]
[source,ts]
----
client.indices.exists({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and aliases. Supports wildcards (`*`).
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`include_defaults` (Optional, boolean)*: If `true`, return all default settings in the response.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.

[discrete]
==== exists_alias
Returns information about whether a particular alias exists.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.existsAlias({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of aliases to check. Supports wildcards (`*`).
** *`index` (Optional, string | string[])*: List of data streams or indices used to limit the request. Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, requests that include a missing data stream or index in the target indices or data streams return an error.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.

[discrete]
==== exists_index_template
Returns information about whether a particular index template exists.

{ref}/indices-templates.html[Endpoint documentation]
[source,ts]
----
client.indices.existsIndexTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: List of index template names used to limit the request. Wildcard (*) expressions are supported.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== explain_data_lifecycle
Retrieves information about the index's current data stream lifecycle, such as any potential encountered error, time since creation etc.

{ref}/data-streams-explain-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.indices.explainDataLifecycle({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: The name of the index to explain
** *`include_defaults` (Optional, boolean)*: indicates if the API should return the default values the system uses for the index's lifecycle
** *`master_timeout` (Optional, string | -1 | 0)*: Specify timeout for connection to master

[discrete]
==== get
Returns information about one or more indices.

{ref}/indices-get-index.html[Endpoint documentation]
[source,ts]
----
client.indices.get({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams, indices, and index aliases used to limit the request.
Wildcard expressions (*) are supported.
** *`allow_no_indices` (Optional, boolean)*: If false, the request returns an error if any wildcard expression, index alias, or _all value targets only
missing or closed indices. This behavior applies even if the request targets other open indices. For example,
a request targeting foo*,bar* returns an error if an index starts with foo but no index starts with bar.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard expressions can match. If the request can target data streams, this argument
determines whether wildcard expressions match hidden data streams. Supports a list of values,
such as open,hidden.
** *`flat_settings` (Optional, boolean)*: If true, returns settings in flat format.
** *`ignore_unavailable` (Optional, boolean)*: If false, requests that target a missing index return an error.
** *`include_defaults` (Optional, boolean)*: If true, return all default settings in the response.
** *`local` (Optional, boolean)*: If true, the request retrieves information from the local node only. Defaults to false, which means information is retrieved from the master node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_alias
Returns an alias.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.getAlias({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: List of aliases to retrieve.
Supports wildcards (`*`).
To retrieve all aliases, omit this parameter or use `*` or `_all`.
** *`index` (Optional, string | string[])*: List of data streams or indices used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.

[discrete]
==== get_data_lifecycle
Returns the data stream lifecycle of the selected data streams.

{ref}/data-streams-get-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.indices.getDataLifecycle({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of data streams to limit the request.
Supports wildcards (`*`).
To target all data streams, omit this parameter or use `*` or `_all`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`include_defaults` (Optional, boolean)*: If `true`, return all default settings in the response.

[discrete]
==== get_data_stream
Returns data streams.

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.getDataStream({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string | string[])*: List of data stream names used to limit the request.
Wildcard (`*`) expressions are supported. If omitted, all data streams are returned.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match.
Supports a list of values, such as `open,hidden`.

[discrete]
==== get_index_template
Returns an index template.

{ref}/indices-templates.html[Endpoint documentation]
[source,ts]
----
client.indices.getIndexTemplate({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: List of index template names used to limit the request. Wildcard (*) expressions are supported.
** *`local` (Optional, boolean)*: If true, the request retrieves information from the local node only. Defaults to false, which means information is retrieved from the master node.
** *`flat_settings` (Optional, boolean)*: If true, returns settings in flat format.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_mapping
Returns mappings for one or more indices.

{ref}/indices-get-mapping.html[Endpoint documentation]
[source,ts]
----
client.indices.getMapping({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_settings
Returns settings for one or more indices.

{ref}/indices-get-settings.html[Endpoint documentation]
[source,ts]
----
client.indices.getSettings({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit
the request. Supports wildcards (`*`). To target all data streams and
indices, omit this parameter or use `*` or `_all`.
** *`name` (Optional, string | string[])*: List or wildcard expression of settings to retrieve.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index
alias, or `_all` value targets only missing or closed indices. This
behavior applies even if the request targets other open indices. For
example, a request targeting `foo*,bar*` returns an error if an index
starts with foo but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`include_defaults` (Optional, boolean)*: If `true`, return all default settings in the response.
** *`local` (Optional, boolean)*: If `true`, the request retrieves information from the local node only. If
`false`, information is retrieved from the master node.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an
error.

[discrete]
==== migrate_to_data_stream
Migrates an alias to a data stream

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.migrateToDataStream({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Name of the index alias to convert to a data stream.

[discrete]
==== modify_data_stream
Modifies a data stream

{ref}/data-streams.html[Endpoint documentation]
[source,ts]
----
client.indices.modifyDataStream({ actions })
----

[discrete]
==== Arguments

* *Request (object):*
** *`actions` ({ add_backing_index, remove_backing_index }[])*: Actions to perform.

[discrete]
==== put_alias
Creates or updates an alias.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.putAlias({ index, name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: List of data streams or indices to add.
Supports wildcards (`*`).
Wildcard patterns that match both data streams and indices return an error.
** *`name` (string)*: Alias to update.
If the alias doesn’t exist, the request creates it.
Index alias names support date math.
** *`filter` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Query used to limit documents the alias can access.
** *`index_routing` (Optional, string)*: Value used to route indexing operations to a specific shard.
If specified, this overwrites the `routing` value for indexing operations.
Data stream aliases don’t support this parameter.
** *`is_write_index` (Optional, boolean)*: If `true`, sets the write index or data stream for the alias.
If an alias points to multiple indices or data streams and `is_write_index` isn’t set, the alias rejects write requests.
If an index alias points to one index and `is_write_index` isn’t set, the index automatically acts as the write index.
Data stream aliases don’t automatically set a write data stream, even if the alias points to one data stream.
** *`routing` (Optional, string)*: Value used to route indexing and search operations to a specific shard.
Data stream aliases don’t support this parameter.
** *`search_routing` (Optional, string)*: Value used to route search operations to a specific shard.
If specified, this overwrites the `routing` value for search operations.
Data stream aliases don’t support this parameter.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== put_data_lifecycle
Updates the data stream lifecycle of the selected data streams.

{ref}/data-streams-put-lifecycle.html[Endpoint documentation]
[source,ts]
----
client.indices.putDataLifecycle({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: List of data streams used to limit the request.
Supports wildcards (`*`).
To target all data streams use `*` or `_all`.
** *`data_retention` (Optional, string | -1 | 0)*: If defined, every document added to this data stream will be stored at least for this time frame.
Any time after this duration the document could be deleted.
When empty, every document in this data stream will be stored indefinitely.
** *`downsampling` (Optional, { rounds })*: If defined, every backing index will execute the configured downsampling configuration after the backing
index is not the data stream write index anymore.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of data stream that wildcard patterns can match.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `hidden`, `open`, `closed`, `none`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an
error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== put_index_template
Creates or updates an index template.

{ref}/indices-templates.html[Endpoint documentation]
[source,ts]
----
client.indices.putIndexTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Index or template name
** *`index_patterns` (Optional, string | string[])*: Name of the index template to create.
** *`composed_of` (Optional, string[])*: An ordered list of component template names.
Component templates are merged in the order specified, meaning that the last component template specified has the highest precedence.
** *`template` (Optional, { aliases, mappings, settings })*: Template to be applied.
It may optionally include an `aliases`, `mappings`, or `settings` configuration.
** *`data_stream` (Optional, { hidden })*: If this object is included, the template is used to create data streams and their backing indices.
Supports an empty object.
Data streams require a matching index template with a `data_stream` object.
** *`priority` (Optional, number)*: Priority to determine index template precedence when a new data stream or index is created.
The index template with the highest priority is chosen.
If no priority is specified the template is treated as though it is of priority 0 (lowest priority).
This number is not automatically generated by Elasticsearch.
** *`version` (Optional, number)*: Version number used to manage index templates externally.
This number is not automatically generated by Elasticsearch.
** *`_meta` (Optional, Record<string, User-defined value>)*: Optional user metadata about the index template.
May have any contents.
This map is not automatically generated by Elasticsearch.
** *`create` (Optional, boolean)*: If `true`, this request cannot replace or update existing index templates.

[discrete]
==== put_mapping
Updates the index mappings.

{ref}/indices-put-mapping.html[Endpoint documentation]
[source,ts]
----
client.indices.putMapping({ index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (string | string[])*: A list of index names the mapping should be added to (supports wildcards); use `_all` or omit to add the mapping on all indices.
** *`date_detection` (Optional, boolean)*: Controls whether dynamic date detection is enabled.
** *`dynamic` (Optional, Enum("strict" | "runtime" | true | false))*: Controls whether new fields are added dynamically.
** *`dynamic_date_formats` (Optional, string[])*: If date detection is enabled then new string fields are checked
against 'dynamic_date_formats' and if the value matches then
a new date field is added instead of string.
** *`dynamic_templates` (Optional, Record<string, { mapping, match, match_mapping_type, match_pattern, path_match, path_unmatch, unmatch }> | Record<string, { mapping, match, match_mapping_type, match_pattern, path_match, path_unmatch, unmatch }>[])*: Specify dynamic templates for the mapping.
** *`_field_names` (Optional, { enabled })*: Control whether field names are enabled for the index.
** *`_meta` (Optional, Record<string, User-defined value>)*: A mapping type can have custom meta data associated with it. These are
not used at all by Elasticsearch, but can be used to store
application-specific metadata.
** *`numeric_detection` (Optional, boolean)*: Automatically map strings into numeric data types for all fields.
** *`properties` (Optional, Record<string, { type } | { boost, fielddata, index, null_value, type } | { type, enabled, null_value, boost, coerce, script, on_script_error, ignore_malformed, time_series_metric, analyzer, eager_global_ordinals, index, index_options, index_phrases, index_prefixes, norms, position_increment_gap, search_analyzer, search_quote_analyzer, term_vector, format, precision_step, locale } | { relations, eager_global_ordinals, type } | { boost, eager_global_ordinals, index, index_options, normalizer, norms, null_value, split_queries_on_whitespace, type } | { type, fields, meta, copy_to } | { type } | { positive_score_impact, type } | { type } | { analyzer, index, index_options, max_shingle_size, norms, search_analyzer, search_quote_analyzer, term_vector, type } | { analyzer, boost, eager_global_ordinals, fielddata, fielddata_frequency_filter, index, index_options, index_phrases, index_prefixes, norms, position_increment_gap, search_analyzer, search_quote_analyzer, term_vector, type } | { type } | { type } | { boost, format, ignore_malformed, index, null_value, precision_step, type } | { boost, fielddata, format, ignore_malformed, index, null_value, precision_step, locale, type } | { type, default_metric, metrics, time_series_metric } | { type, dims, similarity, index, index_options } | { boost, depth_limit, doc_values, eager_global_ordinals, index, index_options, null_value, similarity, split_queries_on_whitespace, type } | { enabled, include_in_parent, include_in_root, type } | { enabled, type } | { analyzer, contexts, max_input_length, preserve_position_increments, preserve_separators, search_analyzer, type } | { value, type } | { path, type } | { ignore_malformed, type } | { boost, index, ignore_malformed, null_value, on_script_error, script, type } | { type } | { analyzer, boost, index, null_value, enable_position_increments, type } | { ignore_malformed, ignore_z_value, null_value, type } | { coerce, ignore_malformed, ignore_z_value, orientation, strategy, type } | { ignore_malformed, ignore_z_value, null_value, type } | { coerce, ignore_malformed, ignore_z_value, orientation, type } | { type, null_value } | { type, null_value } | { type, null_value } | { type, null_value } | { type, null_value } | { type, null_value } | { type, null_value, scaling_factor } | { type, null_value } | { type, null_value } | { format, type } | { type } | { type } | { type } | { type } | { type }>)*: Mapping for a field. For new fields, this mapping can include:

- Field name
- Field data type
- Mapping parameters
** *`_routing` (Optional, { required })*: Enable making a routing value required on indexed documents.
** *`_source` (Optional, { compress, compress_threshold, enabled, excludes, includes, mode })*: Control whether the _source field is enabled on the index.
** *`runtime` (Optional, Record<string, { fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Mapping of runtime fields for the index.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`write_index_only` (Optional, boolean)*: If `true`, the mappings are applied only to the current write index for the target.

[discrete]
==== put_settings
Updates the index settings.

{ref}/indices-update-settings.html[Endpoint documentation]
[source,ts]
----
client.indices.putSettings({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit
the request. Supports wildcards (`*`). To target all data streams and
indices, omit this parameter or use `*` or `_all`.
** *`settings` (Optional, { index, mode, routing_path, soft_deletes, sort, number_of_shards, number_of_replicas, number_of_routing_shards, check_on_startup, codec, routing_partition_size, load_fixed_bitset_filters_eagerly, hidden, auto_expand_replicas, merge, search, refresh_interval, max_result_window, max_inner_result_window, max_rescore_window, max_docvalue_fields_search, max_script_fields, max_ngram_diff, max_shingle_diff, blocks, max_refresh_listeners, analyze, highlight, max_terms_count, max_regex_length, routing, gc_deletes, default_pipeline, final_pipeline, lifecycle, provided_name, creation_date, creation_date_string, uuid, version, verified_before_close, format, max_slices_per_scroll, translog, query_string, priority, top_metrics_max_size, analysis, settings, time_series, shards, queries, similarity, mapping, indexing.slowlog, indexing_pressure, store })*
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index
alias, or `_all` value targets only missing or closed indices. This
behavior applies even if the request targets other open indices. For
example, a request targeting `foo*,bar*` returns an error if an index
starts with `foo` but no index starts with `bar`.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target
data streams, this argument determines whether wildcard expressions match
hidden data streams. Supports a list of values, such as
`open,hidden`.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an
error.
** *`preserve_existing` (Optional, boolean)*: If `true`, existing index settings remain unchanged.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the
 timeout expires, the request fails and returns an error.

[discrete]
==== put_template
Creates or updates an index template.

{ref}/indices-templates.html[Endpoint documentation]
[source,ts]
----
client.indices.putTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the template
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: Aliases for the index.
** *`index_patterns` (Optional, string | string[])*: Array of wildcard expressions used to match the names
of indices during creation.
** *`mappings` (Optional, { all_field, date_detection, dynamic, dynamic_date_formats, dynamic_templates, _field_names, index_field, _meta, numeric_detection, properties, _routing, _size, _source, runtime, enabled })*: Mapping for fields in the index.
** *`order` (Optional, number)*: Order in which Elasticsearch applies this template if index
matches multiple templates.

Templates with lower 'order' values are merged first. Templates with higher
'order' values are merged later, overriding templates with lower values.
** *`settings` (Optional, Record<string, User-defined value>)*: Configuration options for the index.
** *`version` (Optional, number)*: Version number used to manage index templates externally. This number
is not automatically generated by Elasticsearch.
** *`create` (Optional, boolean)*: If true, this request cannot replace or update existing index templates.
** *`flat_settings` (Optional, boolean)*: If `true`, returns settings in flat format.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is
received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== refresh
Performs the refresh operation in one or more indices.

{ref}/indices-refresh.html[Endpoint documentation]
[source,ts]
----
client.indices.refresh({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases used to limit the request.
Supports wildcards (`*`).
To target all data streams and indices, omit this parameter or use `*` or `_all`.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.

[discrete]
==== resolve_index
Returns information about any matching indices, aliases, and data streams

{ref}/indices-resolve-index-api.html[Endpoint documentation]
[source,ts]
----
client.indices.resolveIndex({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string | string[])*: Comma-separated name(s) or index pattern(s) of the indices, aliases, and data streams to resolve.
Resources on remote clusters can be specified using the `<cluster>`:`<name>` syntax.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.

[discrete]
==== rollover
Updates an alias to point to a new index when the existing index
is considered to be too large or too old.

{ref}/indices-rollover-index.html[Endpoint documentation]
[source,ts]
----
client.indices.rollover({ alias })
----

[discrete]
==== Arguments

* *Request (object):*
** *`alias` (string)*: Name of the data stream or index alias to roll over.
** *`new_index` (Optional, string)*: Name of the index to create.
Supports date math.
Data streams do not support this parameter.
** *`aliases` (Optional, Record<string, { filter, index_routing, is_hidden, is_write_index, routing, search_routing }>)*: Aliases for the target index.
Data streams do not support this parameter.
** *`conditions` (Optional, { min_age, max_age, max_age_millis, min_docs, max_docs, max_size, max_size_bytes, min_size, min_size_bytes, max_primary_shard_size, max_primary_shard_size_bytes, min_primary_shard_size, min_primary_shard_size_bytes, max_primary_shard_docs, min_primary_shard_docs })*: Conditions for the rollover.
If specified, Elasticsearch only performs the rollover if the current index satisfies these conditions.
If this parameter is not specified, Elasticsearch performs the rollover unconditionally.
If conditions are specified, at least one of them must be a `max_*` condition.
The index will rollover if any `max_*` condition is satisfied and all `min_*` conditions are satisfied.
** *`mappings` (Optional, { all_field, date_detection, dynamic, dynamic_date_formats, dynamic_templates, _field_names, index_field, _meta, numeric_detection, properties, _routing, _size, _source, runtime, enabled })*: Mapping for fields in the index.
If specified, this mapping can include field names, field data types, and mapping paramaters.
** *`settings` (Optional, Record<string, User-defined value>)*: Configuration options for the index.
Data streams do not support this parameter.
** *`dry_run` (Optional, boolean)*: If `true`, checks whether the current index satisfies the specified conditions but does not perform a rollover.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_active_shards` (Optional, number | Enum("all" | "index-setting"))*: The number of shard copies that must be active before proceeding with the operation.
Set to all or any positive integer up to the total number of shards in the index (`number_of_replicas+1`).

[discrete]
==== simulate_index_template
Simulate matching the given index name against the index templates in the system

{ref}/indices-templates.html[Endpoint documentation]
[source,ts]
----
client.indices.simulateIndexTemplate({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: Index or template name to simulate
** *`allow_auto_create` (Optional, boolean)*: This setting overrides the value of the `action.auto_create_index` cluster setting.
If set to `true` in a template, then indices can be automatically created using that template even if auto-creation of indices is disabled via `actions.auto_create_index`.
If set to `false`, then indices or data streams matching the template must always be explicitly created, and may never be automatically created.
** *`index_patterns` (Optional, string | string[])*: Array of wildcard (`*`) expressions used to match the names of data streams and indices during creation.
** *`composed_of` (Optional, string[])*: An ordered list of component template names.
Component templates are merged in the order specified, meaning that the last component template specified has the highest precedence.
** *`template` (Optional, { aliases, mappings, settings })*: Template to be applied.
It may optionally include an `aliases`, `mappings`, or `settings` configuration.
** *`data_stream` (Optional, { hidden })*: If this object is included, the template is used to create data streams and their backing indices.
Supports an empty object.
Data streams require a matching index template with a `data_stream` object.
** *`priority` (Optional, number)*: Priority to determine index template precedence when a new data stream or index is created.
The index template with the highest priority is chosen.
If no priority is specified the template is treated as though it is of priority 0 (lowest priority).
This number is not automatically generated by Elasticsearch.
** *`version` (Optional, number)*: Version number used to manage index templates externally.
This number is not automatically generated by Elasticsearch.
** *`_meta` (Optional, Record<string, User-defined value>)*: Optional user metadata about the index template.
May have any contents.
This map is not automatically generated by Elasticsearch.
** *`create` (Optional, boolean)*: If `true`, the template passed in the body is only used if no existing
templates match the same index patterns. If `false`, the simulation uses
the template with the highest priority. Note that the template is not
permanently added or updated in either case; it is only used for the
simulation.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received
before the timeout expires, the request fails and returns an error.

[discrete]
==== simulate_template
Simulate resolving the given template name or body

{ref}/indices-templates.html[Endpoint documentation]
[source,ts]
----
client.indices.simulateTemplate({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string)*: Name of the index template to simulate. To test a template configuration before you add it to the cluster, omit
this parameter and specify the template configuration in the request body.
** *`template` (Optional, { index_patterns, composed_of, template, version, priority, _meta, allow_auto_create, data_stream })*
** *`create` (Optional, boolean)*: If true, the template passed in the body is only used if no existing templates match the same index patterns. If false, the simulation uses the template with the highest priority. Note that the template is not permanently added or updated in either case; it is only used for the simulation.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== update_aliases
Updates index aliases.

{ref}/indices-aliases.html[Endpoint documentation]
[source,ts]
----
client.indices.updateAliases({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`actions` (Optional, { add_backing_index, remove_backing_index }[])*: Actions to perform.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== validate_query
Allows a user to validate a potentially expensive query without executing it.

{ref}/search-validate.html[Endpoint documentation]
[source,ts]
----
client.indices.validateQuery({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`index` (Optional, string | string[])*: List of data streams, indices, and aliases to search.
Supports wildcards (`*`).
To search all data streams or indices, omit this parameter or use `*` or `_all`.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Query in the Lucene query string syntax.
** *`allow_no_indices` (Optional, boolean)*: If `false`, the request returns an error if any wildcard expression, index alias, or `_all` value targets only missing or closed indices.
This behavior applies even if the request targets other open indices.
** *`all_shards` (Optional, boolean)*: If `true`, the validation is executed on all shards instead of one random shard per index.
** *`analyzer` (Optional, string)*: Analyzer to use for the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`analyze_wildcard` (Optional, boolean)*: If `true`, wildcard and prefix queries are analyzed.
** *`default_operator` (Optional, Enum("and" | "or"))*: The default operator for query string query: `AND` or `OR`.
** *`df` (Optional, string)*: Field to use as default where no field prefix is given in the query string.
This parameter can only be used when the `q` query string parameter is specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match.
If the request can target data streams, this argument determines whether wildcard expressions match hidden data streams.
Supports a list of values, such as `open,hidden`.
Valid values are: `all`, `open`, `closed`, `hidden`, `none`.
** *`explain` (Optional, boolean)*: If `true`, the response returns detailed information if an error has occurred.
** *`ignore_unavailable` (Optional, boolean)*: If `false`, the request returns an error if it targets a missing or closed index.
** *`lenient` (Optional, boolean)*: If `true`, format-based query failures (such as providing text to a numeric field) in the query string will be ignored.
** *`rewrite` (Optional, boolean)*: If `true`, returns a more detailed explanation showing the actual Lucene query that will be executed.
** *`q` (Optional, string)*: Query in the Lucene query string syntax.

[discrete]
=== ingest
[discrete]
==== delete_pipeline
Deletes a pipeline.

{ref}/delete-pipeline-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.deletePipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Pipeline ID or wildcard expression of pipeline IDs used to limit the request.
To delete all ingest pipelines in a cluster, use a value of `*`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_pipeline
Returns a pipeline.

{ref}/get-pipeline-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.getPipeline({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: List of pipeline IDs to retrieve.
Wildcard (`*`) expressions are supported.
To get all ingest pipelines, omit this parameter or use `*`.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node.
If no response is received before the timeout expires, the request fails and returns an error.
** *`summary` (Optional, boolean)*: Return pipelines without their definitions (default: false)

[discrete]
==== processor_grok
Returns a list of the built-in patterns.

{ref}/grok-processor.html[Endpoint documentation]
[source,ts]
----
client.ingest.processorGrok()
----


[discrete]
==== put_pipeline
Creates or updates a pipeline.

{ref}/ingest.html[Endpoint documentation]
[source,ts]
----
client.ingest.putPipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: ID of the ingest pipeline to create or update.
** *`_meta` (Optional, Record<string, User-defined value>)*: Optional metadata about the ingest pipeline. May have any contents. This map is not automatically generated by Elasticsearch.
** *`description` (Optional, string)*: Description of the ingest pipeline.
** *`on_failure` (Optional, { attachment, append, csv, convert, date, date_index_name, dot_expander, enrich, fail, foreach, json, user_agent, kv, geoip, grok, gsub, join, lowercase, remove, rename, script, set, sort, split, trim, uppercase, urldecode, bytes, dissect, set_security_user, pipeline, drop, circle, inference }[])*: Processors to run immediately after a processor failure. Each processor supports a processor-level `on_failure` value. If a processor without an `on_failure` value fails, Elasticsearch uses this pipeline-level parameter as a fallback. The processors in this parameter run sequentially in the order specified. Elasticsearch will not attempt to run the pipeline's remaining processors.
** *`processors` (Optional, { attachment, append, csv, convert, date, date_index_name, dot_expander, enrich, fail, foreach, json, user_agent, kv, geoip, grok, gsub, join, lowercase, remove, rename, script, set, sort, split, trim, uppercase, urldecode, bytes, dissect, set_security_user, pipeline, drop, circle, inference }[])*: Processors used to perform transformations on documents before indexing. Processors run sequentially in the order specified.
** *`version` (Optional, number)*: Version number used by external systems to track ingest pipelines. This parameter is intended for external systems only. Elasticsearch does not use or validate pipeline version numbers.
** *`master_timeout` (Optional, string | -1 | 0)*: Period to wait for a connection to the master node. If no response is received before the timeout expires, the request fails and returns an error.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.
** *`if_version` (Optional, number)*: Required version for optimistic concurrency control for pipeline updates

[discrete]
==== simulate
Allows to simulate a pipeline with example documents.

{ref}/simulate-pipeline-api.html[Endpoint documentation]
[source,ts]
----
client.ingest.simulate({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Pipeline to test.
If you don’t specify a `pipeline` in the request body, this parameter is required.
** *`docs` (Optional, { _id, _index, _source }[])*: Sample documents to test in the pipeline.
** *`pipeline` (Optional, { description, on_failure, processors, version })*: Pipeline to test.
If you don’t specify the `pipeline` request path parameter, this parameter is required.
If you specify both this and the request path parameter, the API only uses the request path parameter.
** *`verbose` (Optional, boolean)*: If `true`, the response includes output data for each processor in the executed pipeline.

[discrete]
=== license
[discrete]
==== get
Retrieves licensing information for the cluster

{ref}/get-license.html[Endpoint documentation]
[source,ts]
----
client.license.get({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`accept_enterprise` (Optional, boolean)*: If `true`, this parameter returns enterprise for Enterprise license types. If `false`, this parameter returns platinum for both platinum and enterprise license types. This behavior is maintained for backwards compatibility.
This parameter is deprecated and will always be set to true in 8.x.
** *`local` (Optional, boolean)*: Specifies whether to retrieve local information. The default value is `false`, which means the information is retrieved from the master node.

[discrete]
=== logstash
[discrete]
==== delete_pipeline
Deletes Logstash Pipelines used by Central Management

{ref}/logstash-api-delete-pipeline.html[Endpoint documentation]
[source,ts]
----
client.logstash.deletePipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the pipeline.

[discrete]
==== get_pipeline
Retrieves Logstash Pipelines used by Central Management

{ref}/logstash-api-get-pipeline.html[Endpoint documentation]
[source,ts]
----
client.logstash.getPipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string | string[])*: List of pipeline identifiers.

[discrete]
==== put_pipeline
Adds and updates Logstash Pipelines used for Central Management

{ref}/logstash-api-put-pipeline.html[Endpoint documentation]
[source,ts]
----
client.logstash.putPipeline({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the pipeline.
** *`pipeline` (Optional, { description, on_failure, processors, version })*

[discrete]
=== ml
[discrete]
==== close_job
Closes one or more anomaly detection jobs. A job can be opened and closed multiple times throughout its lifecycle.

{ref}/ml-close-job.html[Endpoint documentation]
[source,ts]
----
client.ml.closeJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job. It can be a job identifier, a group name, or a wildcard expression. You can close multiple anomaly detection jobs in a single API request by using a group name, a list of jobs, or a wildcard expression. You can close all jobs by using `_all` or by specifying `*` as the job identifier.
** *`allow_no_match` (Optional, boolean)*: Refer to the description for the `allow_no_match` query parameter.
** *`force` (Optional, boolean)*: Refer to the descriptiion for the `force` query parameter.
** *`timeout` (Optional, string | -1 | 0)*: Refer to the description for the `timeout` query parameter.

[discrete]
==== delete_calendar
Deletes a calendar.

{ref}/ml-delete-calendar.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteCalendar({ calendar_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.

[discrete]
==== delete_calendar_event
Deletes scheduled events from a calendar.

{ref}/ml-delete-calendar-event.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteCalendarEvent({ calendar_id, event_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`event_id` (string)*: Identifier for the scheduled event.
You can obtain this identifier by using the get calendar events API.

[discrete]
==== delete_calendar_job
Deletes anomaly detection jobs from a calendar.

{ref}/ml-delete-calendar-job.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteCalendarJob({ calendar_id, job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`job_id` (string | string[])*: An identifier for the anomaly detection jobs. It can be a job identifier, a group name, or a
list of jobs or groups.

[discrete]
==== delete_data_frame_analytics
Deletes an existing data frame analytics job.

{ref}/delete-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteDataFrameAnalytics({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job.
** *`force` (Optional, boolean)*: If `true`, it deletes a job that is not stopped; this method is quicker than stopping and deleting the job.
** *`timeout` (Optional, string | -1 | 0)*: The time to wait for the job to be deleted.

[discrete]
==== delete_datafeed
Deletes an existing datafeed.

{ref}/ml-delete-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: A numerical character string that uniquely identifies the datafeed. This
identifier can contain lowercase alphanumeric characters (a-z and 0-9),
hyphens, and underscores. It must start and end with alphanumeric
characters.
** *`force` (Optional, boolean)*: Use to forcefully delete a started datafeed; this method is quicker than
stopping and deleting the datafeed.

[discrete]
==== delete_filter
Deletes a filter.

{ref}/ml-delete-filter.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteFilter({ filter_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`filter_id` (string)*: A string that uniquely identifies a filter.

[discrete]
==== delete_job
Deletes an existing anomaly detection job.

{ref}/ml-delete-job.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`force` (Optional, boolean)*: Use to forcefully delete an opened job; this method is quicker than
closing and deleting the job.
** *`delete_user_annotations` (Optional, boolean)*: Specifies whether annotations that have been added by the
user should be deleted along with any auto-generated annotations when the job is
reset.
** *`wait_for_completion` (Optional, boolean)*: Specifies whether the request should return immediately or wait until the
job deletion completes.

[discrete]
==== delete_trained_model
Deletes an existing trained inference model that is currently not referenced by an ingest pipeline.

{ref}/delete-trained-models.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteTrainedModel({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`force` (Optional, boolean)*: Forcefully deletes a trained model that is referenced by ingest pipelines or has a started deployment.

[discrete]
==== delete_trained_model_alias
Deletes a model alias that refers to the trained model

{ref}/delete-trained-models-aliases.html[Endpoint documentation]
[source,ts]
----
client.ml.deleteTrainedModelAlias({ model_alias, model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_alias` (string)*: The model alias to delete.
** *`model_id` (string)*: The trained model ID to which the model alias refers.

[discrete]
==== estimate_model_memory
Estimates the model memory

{ref}/ml-apis.html[Endpoint documentation]
[source,ts]
----
client.ml.estimateModelMemory({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`analysis_config` (Optional, { bucket_span, categorization_analyzer, categorization_field_name, categorization_filters, detectors, influencers, latency, model_prune_window, multivariate_by_fields, per_partition_categorization, summary_count_field_name })*: For a list of the properties that you can specify in the
`analysis_config` component of the body of this API.
** *`max_bucket_cardinality` (Optional, Record<string, number>)*: Estimates of the highest cardinality in a single bucket that is observed
for influencer fields over the time period that the job analyzes data.
To produce a good answer, values must be provided for all influencer
fields. Providing values for fields that are not listed as `influencers`
has no effect on the estimation.
** *`overall_cardinality` (Optional, Record<string, number>)*: Estimates of the cardinality that is observed for fields over the whole
time period that the job analyzes data. To produce a good answer, values
must be provided for fields referenced in the `by_field_name`,
`over_field_name` and `partition_field_name` of any detectors. Providing
values for other fields has no effect on the estimation. It can be
omitted from the request if no detectors have a `by_field_name`,
`over_field_name` or `partition_field_name`.

[discrete]
==== evaluate_data_frame
Evaluates the data frame analytics for an annotated index.

{ref}/evaluate-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.evaluateDataFrame({ evaluation, index })
----

[discrete]
==== Arguments

* *Request (object):*
** *`evaluation` ({ classification, outlier_detection, regression })*: Defines the type of evaluation you want to perform.
** *`index` (string)*: Defines the `index` in which the evaluation will be performed.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: A query clause that retrieves a subset of data from the source index.

[discrete]
==== flush_job
Forces any buffered data to be processed by the job.

{ref}/ml-flush-job.html[Endpoint documentation]
[source,ts]
----
client.ml.flushJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`advance_time` (Optional, string | Unit)*: Refer to the description for the `advance_time` query parameter.
** *`calc_interim` (Optional, boolean)*: Refer to the description for the `calc_interim` query parameter.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`skip_time` (Optional, string | Unit)*: Refer to the description for the `skip_time` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.

[discrete]
==== get_calendar_events
Retrieves information about the scheduled events in calendars.

{ref}/ml-get-calendar-event.html[Endpoint documentation]
[source,ts]
----
client.ml.getCalendarEvents({ calendar_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar. You can get information for multiple calendars by using a list of ids or a wildcard expression. You can get information for all calendars by using `_all` or `*` or by omitting the calendar identifier.
** *`end` (Optional, string | Unit)*: Specifies to get events with timestamps earlier than this time.
** *`from` (Optional, number)*: Skips the specified number of events.
** *`job_id` (Optional, string)*: Specifies to get events for a specific anomaly detection job identifier or job group. It must be used with a calendar identifier of `_all` or `*`.
** *`size` (Optional, number)*: Specifies the maximum number of events to obtain.
** *`start` (Optional, string | Unit)*: Specifies to get events with timestamps after this time.

[discrete]
==== get_calendars
Retrieves configuration information for calendars.

{ref}/ml-get-calendar.html[Endpoint documentation]
[source,ts]
----
client.ml.getCalendars({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (Optional, string)*: A string that uniquely identifies a calendar. You can get information for multiple calendars by using a list of ids or a wildcard expression. You can get information for all calendars by using `_all` or `*` or by omitting the calendar identifier.
** *`page` (Optional, { from, size })*: This object is supported only when you omit the calendar identifier.
** *`from` (Optional, number)*: Skips the specified number of calendars. This parameter is supported only when you omit the calendar identifier.
** *`size` (Optional, number)*: Specifies the maximum number of calendars to obtain. This parameter is supported only when you omit the calendar identifier.

[discrete]
==== get_data_frame_analytics
Retrieves configuration information for data frame analytics jobs.

{ref}/get-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.getDataFrameAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the data frame analytics job. If you do not specify this
option, the API returns information for the first hundred data frame
analytics jobs.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no data frame analytics
jobs that match.
2. Contains the `_all` string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value returns an empty data_frame_analytics array when there
are no matches and the subset of results when there are partial matches.
If this parameter is `false`, the request returns a 404 status code when
there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of data frame analytics jobs.
** *`size` (Optional, number)*: Specifies the maximum number of data frame analytics jobs to obtain.
** *`exclude_generated` (Optional, boolean)*: Indicates if certain fields should be removed from the configuration on
retrieval. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.

[discrete]
==== get_data_frame_analytics_stats
Retrieves usage information for data frame analytics jobs.

{ref}/get-dfanalytics-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getDataFrameAnalyticsStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the data frame analytics job. If you do not specify this
option, the API returns information for the first hundred data frame
analytics jobs.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no data frame analytics
jobs that match.
2. Contains the `_all` string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value returns an empty data_frame_analytics array when there
are no matches and the subset of results when there are partial matches.
If this parameter is `false`, the request returns a 404 status code when
there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of data frame analytics jobs.
** *`size` (Optional, number)*: Specifies the maximum number of data frame analytics jobs to obtain.
** *`verbose` (Optional, boolean)*: Defines whether the stats response should be verbose.

[discrete]
==== get_datafeed_stats
Retrieves usage information for datafeeds.

{ref}/ml-get-datafeed-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getDatafeedStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (Optional, string | string[])*: Identifier for the datafeed. It can be a datafeed identifier or a
wildcard expression. If you do not specify one of these options, the API
returns information about all datafeeds.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no datafeeds that match.
2. Contains the `_all` string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value is `true`, which returns an empty `datafeeds` array
when there are no matches and the subset of results when there are
partial matches. If this parameter is `false`, the request returns a
`404` status code when there are no matches or only partial matches.

[discrete]
==== get_datafeeds
Retrieves configuration information for datafeeds.

{ref}/ml-get-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.getDatafeeds({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (Optional, string | string[])*: Identifier for the datafeed. It can be a datafeed identifier or a
wildcard expression. If you do not specify one of these options, the API
returns information about all datafeeds.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no datafeeds that match.
2. Contains the `_all` string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value is `true`, which returns an empty `datafeeds` array
when there are no matches and the subset of results when there are
partial matches. If this parameter is `false`, the request returns a
`404` status code when there are no matches or only partial matches.
** *`exclude_generated` (Optional, boolean)*: Indicates if certain fields should be removed from the configuration on
retrieval. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.

[discrete]
==== get_filters
Retrieves filters.

{ref}/ml-get-filter.html[Endpoint documentation]
[source,ts]
----
client.ml.getFilters({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`filter_id` (Optional, string | string[])*: A string that uniquely identifies a filter.
** *`from` (Optional, number)*: Skips the specified number of filters.
** *`size` (Optional, number)*: Specifies the maximum number of filters to obtain.

[discrete]
==== get_job_stats
Retrieves usage information for anomaly detection jobs.

{ref}/ml-get-job-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getJobStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (Optional, string)*: Identifier for the anomaly detection job. It can be a job identifier, a
group name, a list of jobs, or a wildcard expression. If
you do not specify one of these options, the API returns information for
all anomaly detection jobs.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no jobs that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

If `true`, the API returns an empty `jobs` array when
there are no matches and the subset of results when there are partial
matches. If `false`, the API returns a `404` status
code when there are no matches or only partial matches.

[discrete]
==== get_jobs
Retrieves configuration information for anomaly detection jobs.

{ref}/ml-get-job.html[Endpoint documentation]
[source,ts]
----
client.ml.getJobs({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (Optional, string | string[])*: Identifier for the anomaly detection job. It can be a job identifier, a
group name, or a wildcard expression. If you do not specify one of these
options, the API returns information for all anomaly detection jobs.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no jobs that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value is `true`, which returns an empty `jobs` array when
there are no matches and the subset of results when there are partial
matches. If this parameter is `false`, the request returns a `404` status
code when there are no matches or only partial matches.
** *`exclude_generated` (Optional, boolean)*: Indicates if certain fields should be removed from the configuration on
retrieval. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.

[discrete]
==== get_overall_buckets
Retrieves overall bucket results that summarize the bucket results of multiple anomaly detection jobs.

{ref}/ml-get-overall-buckets.html[Endpoint documentation]
[source,ts]
----
client.ml.getOverallBuckets({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job. It can be a job identifier, a
group name, a list of jobs or groups, or a wildcard
expression.

You can summarize the bucket results for all anomaly detection jobs by
using `_all` or by specifying `*` as the `<job_id>`.
** *`allow_no_match` (Optional, boolean)*: Refer to the description for the `allow_no_match` query parameter.
** *`bucket_span` (Optional, string | -1 | 0)*: Refer to the description for the `bucket_span` query parameter.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`exclude_interim` (Optional, boolean)*: Refer to the description for the `exclude_interim` query parameter.
** *`overall_score` (Optional, number | string)*: Refer to the description for the `overall_score` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.
** *`top_n` (Optional, number)*: Refer to the description for the `top_n` query parameter.

[discrete]
==== get_trained_models
Retrieves configuration information for a trained inference model.

{ref}/get-trained-models.html[Endpoint documentation]
[source,ts]
----
client.ml.getTrainedModels({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (Optional, string)*: The unique identifier of the trained model.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

- Contains wildcard expressions and there are no models that match.
- Contains the _all string or no identifiers and there are no matches.
- Contains wildcard expressions and there are only partial matches.

If true, it returns an empty array when there are no matches and the
subset of results when there are partial matches.
** *`decompress_definition` (Optional, boolean)*: Specifies whether the included model definition should be returned as a
JSON map (true) or in a custom compressed format (false).
** *`exclude_generated` (Optional, boolean)*: Indicates if certain fields should be removed from the configuration on
retrieval. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.
** *`from` (Optional, number)*: Skips the specified number of models.
** *`include` (Optional, Enum("definition" | "feature_importance_baseline" | "hyperparameters" | "total_feature_importance" | "definition_status"))*: A comma delimited string of optional fields to include in the response
body.
** *`size` (Optional, number)*: Specifies the maximum number of models to obtain.
** *`tags` (Optional, string)*: A comma delimited string of tags. A trained model can have many tags, or
none. When supplied, only trained models that contain all the supplied
tags are returned.

[discrete]
==== get_trained_models_stats
Retrieves usage information for trained inference models.

{ref}/get-trained-models-stats.html[Endpoint documentation]
[source,ts]
----
client.ml.getTrainedModelsStats({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (Optional, string | string[])*: The unique identifier of the trained model or a model alias. It can be a
list or a wildcard expression.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

- Contains wildcard expressions and there are no models that match.
- Contains the _all string or no identifiers and there are no matches.
- Contains wildcard expressions and there are only partial matches.

If true, it returns an empty array when there are no matches and the
subset of results when there are partial matches.
** *`from` (Optional, number)*: Skips the specified number of models.
** *`size` (Optional, number)*: Specifies the maximum number of models to obtain.

[discrete]
==== infer_trained_model
Evaluate a trained model.

{ref}/infer-trained-model.html[Endpoint documentation]
[source,ts]
----
client.ml.inferTrainedModel({ model_id, docs })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`docs` (Record<string, User-defined value>[])*: An array of objects to pass to the model for inference. The objects should contain a fields matching your
configured trained model input. Typically, for NLP models, the field name is `text_field`.
Currently, for NLP models, only a single value is allowed.
** *`inference_config` (Optional, { regression, classification, text_classification, zero_shot_classification, fill_mask, ner, pass_through, text_embedding, text_expansion, question_answering })*: The inference configuration updates to apply on the API call
** *`timeout` (Optional, string | -1 | 0)*: Controls the amount of time to wait for inference results.

[discrete]
==== open_job
Opens one or more anomaly detection jobs.

{ref}/ml-open-job.html[Endpoint documentation]
[source,ts]
----
client.ml.openJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the anomaly detection job.
** *`timeout` (Optional, string | -1 | 0)*: Refer to the description for the `timeout` query parameter.

[discrete]
==== post_calendar_events
Posts scheduled events in a calendar.

{ref}/ml-post-calendar-event.html[Endpoint documentation]
[source,ts]
----
client.ml.postCalendarEvents({ calendar_id, events })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`events` ({ calendar_id, event_id, description, end_time, start_time }[])*: A list of one of more scheduled events. The event’s start and end times can be specified as integer milliseconds since the epoch or as a string in ISO 8601 format.

[discrete]
==== preview_data_frame_analytics
Previews that will be analyzed given a data frame analytics config.

{ref}/preview-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.previewDataFrameAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: Identifier for the data frame analytics job.
** *`config` (Optional, { source, analysis, model_memory_limit, max_num_threads, analyzed_fields })*: A data frame analytics config as described in create data frame analytics
jobs. Note that `id` and `dest` don’t need to be provided in the context of
this API.

[discrete]
==== preview_datafeed
Previews a datafeed.

{ref}/ml-preview-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.previewDatafeed({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (Optional, string)*: A numerical character string that uniquely identifies the datafeed. This identifier can contain lowercase
alphanumeric characters (a-z and 0-9), hyphens, and underscores. It must start and end with alphanumeric
characters. NOTE: If you use this path parameter, you cannot provide datafeed or anomaly detection job
configuration details in the request body.
** *`datafeed_config` (Optional, { aggregations, chunking_config, datafeed_id, delayed_data_check_config, frequency, indices, indices_options, job_id, max_empty_searches, query, query_delay, runtime_mappings, script_fields, scroll_size })*: The datafeed definition to preview.
** *`job_config` (Optional, { allow_lazy_open, analysis_config, analysis_limits, background_persist_interval, custom_settings, daily_model_snapshot_retention_after_days, data_description, datafeed_config, description, groups, job_id, job_type, model_plot_config, model_snapshot_retention_days, renormalization_window_days, results_index_name, results_retention_days })*: The configuration details for the anomaly detection job that is associated with the datafeed. If the
`datafeed_config` object does not include a `job_id` that references an existing anomaly detection job, you must
supply this `job_config` object. If you include both a `job_id` and a `job_config`, the latter information is
used. You cannot specify a `job_config` object unless you also supply a `datafeed_config` object.
** *`start` (Optional, string | Unit)*: The start time from where the datafeed preview should begin
** *`end` (Optional, string | Unit)*: The end time when the datafeed preview should stop

[discrete]
==== put_calendar
Instantiates a calendar.

{ref}/ml-put-calendar.html[Endpoint documentation]
[source,ts]
----
client.ml.putCalendar({ calendar_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`job_ids` (Optional, string[])*: An array of anomaly detection job identifiers.
** *`description` (Optional, string)*: A description of the calendar.

[discrete]
==== put_calendar_job
Adds an anomaly detection job to a calendar.

{ref}/ml-put-calendar-job.html[Endpoint documentation]
[source,ts]
----
client.ml.putCalendarJob({ calendar_id, job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`calendar_id` (string)*: A string that uniquely identifies a calendar.
** *`job_id` (string)*: An identifier for the anomaly detection jobs. It can be a job identifier, a group name, or a list of jobs or groups.

[discrete]
==== put_data_frame_analytics
Instantiates a data frame analytics job.

{ref}/put-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.putDataFrameAnalytics({ id, analysis, dest, source })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`analysis` ({ classification, outlier_detection, regression })*: The analysis configuration, which contains the information necessary to
perform one of the following types of analysis: classification, outlier
detection, or regression.
** *`dest` ({ index, results_field })*: The destination configuration.
** *`source` ({ index, query, runtime_mappings, _source })*: The configuration of how to source the analysis data.
** *`allow_lazy_start` (Optional, boolean)*: Specifies whether this job can start when there is insufficient machine
learning node capacity for it to be immediately assigned to a node. If
set to `false` and a machine learning node with capacity to run the job
cannot be immediately found, the API returns an error. If set to `true`,
the API does not return an error; the job waits in the `starting` state
until sufficient machine learning node capacity is available. This
behavior is also affected by the cluster-wide
`xpack.ml.max_lazy_ml_nodes` setting.
** *`analyzed_fields` (Optional, { includes, excludes })*: Specifies `includes` and/or `excludes` patterns to select which fields
will be included in the analysis. The patterns specified in `excludes`
are applied last, therefore `excludes` takes precedence. In other words,
if the same field is specified in both `includes` and `excludes`, then
the field will not be included in the analysis. If `analyzed_fields` is
not set, only the relevant fields will be included. For example, all the
numeric fields for outlier detection.
The supported fields vary for each type of analysis. Outlier detection
requires numeric or `boolean` data to analyze. The algorithms don’t
support missing values therefore fields that have data types other than
numeric or boolean are ignored. Documents where included fields contain
missing values, null values, or an array are also ignored. Therefore the
`dest` index may contain documents that don’t have an outlier score.
Regression supports fields that are numeric, `boolean`, `text`,
`keyword`, and `ip` data types. It is also tolerant of missing values.
Fields that are supported are included in the analysis, other fields are
ignored. Documents where included fields contain an array with two or
more values are also ignored. Documents in the `dest` index that don’t
contain a results field are not included in the regression analysis.
Classification supports fields that are numeric, `boolean`, `text`,
`keyword`, and `ip` data types. It is also tolerant of missing values.
Fields that are supported are included in the analysis, other fields are
ignored. Documents where included fields contain an array with two or
more values are also ignored. Documents in the `dest` index that don’t
contain a results field are not included in the classification analysis.
Classification analysis can be improved by mapping ordinal variable
values to a single number. For example, in case of age ranges, you can
model the values as `0-14 = 0`, `15-24 = 1`, `25-34 = 2`, and so on.
** *`description` (Optional, string)*: A description of the job.
** *`max_num_threads` (Optional, number)*: The maximum number of threads to be used by the analysis. Using more
threads may decrease the time necessary to complete the analysis at the
cost of using more CPU. Note that the process may use additional threads
for operational functionality other than the analysis itself.
** *`model_memory_limit` (Optional, string)*: The approximate maximum amount of memory resources that are permitted for
analytical processing. If your `elasticsearch.yml` file contains an
`xpack.ml.max_model_memory_limit` setting, an error occurs when you try
to create data frame analytics jobs that have `model_memory_limit` values
greater than that setting.

[discrete]
==== put_datafeed
Instantiates a datafeed.

{ref}/ml-put-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.putDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: A numerical character string that uniquely identifies the datafeed.
This identifier can contain lowercase alphanumeric characters (a-z and 0-9), hyphens, and underscores.
It must start and end with alphanumeric characters.
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, cardinality, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: If set, the datafeed performs aggregation searches.
Support for aggregations is limited and should be used only with low cardinality data.
** *`chunking_config` (Optional, { mode, time_span })*: Datafeeds might be required to search over long time periods, for several months or years.
This search is split into time chunks in order to ensure the load on Elasticsearch is managed.
Chunking configuration controls how the size of these time chunks are calculated;
it is an advanced configuration option.
** *`delayed_data_check_config` (Optional, { check_window, enabled })*: Specifies whether the datafeed checks for missing data and the size of the window.
The datafeed can optionally search over indices that have already been read in an effort to determine whether
any data has subsequently been added to the index. If missing data is found, it is a good indication that the
`query_delay` is set too low and the data is being indexed after the datafeed has passed that moment in time.
This check runs only on real-time datafeeds.
** *`frequency` (Optional, string | -1 | 0)*: The interval at which scheduled queries are made while the datafeed runs in real time.
The default value is either the bucket span for short bucket spans, or, for longer bucket spans, a sensible
fraction of the bucket span. When `frequency` is shorter than the bucket span, interim results for the last
(partial) bucket are written then eventually overwritten by the full bucket results. If the datafeed uses
aggregations, this value must be divisible by the interval of the date histogram aggregation.
** *`indices` (Optional, string | string[])*: An array of index names. Wildcards are supported. If any of the indices are in remote clusters, the machine
learning nodes must have the `remote_cluster_client` role.
** *`indices_options` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, ignore_throttled })*: Specifies index expansion options that are used during search
** *`job_id` (Optional, string)*: Identifier for the anomaly detection job.
** *`max_empty_searches` (Optional, number)*: If a real-time datafeed has never seen any data (including during any initial training period), it automatically
stops and closes the associated job after this many real-time searches return no documents. In other words,
it stops after `frequency` times `max_empty_searches` of real-time operation. If not set, a datafeed with no
end time that sees no data remains started until it is explicitly stopped. By default, it is not set.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: The Elasticsearch query domain-specific language (DSL). This value corresponds to the query object in an
Elasticsearch search POST body. All the options that are supported by Elasticsearch can be used, as this
object is passed verbatim to Elasticsearch.
** *`query_delay` (Optional, string | -1 | 0)*: The number of seconds behind real time that data is queried. For example, if data from 10:04 a.m. might
not be searchable in Elasticsearch until 10:06 a.m., set this property to 120 seconds. The default
value is randomly selected between `60s` and `120s`. This randomness improves the query performance
when there are multiple jobs running on the same node.
** *`runtime_mappings` (Optional, Record<string, { fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Specifies runtime fields for the datafeed search.
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Specifies scripts that evaluate custom expressions and returns script fields to the datafeed.
The detector configuration objects in a job can contain functions that use these script fields.
** *`scroll_size` (Optional, number)*: The size parameter that is used in Elasticsearch searches when the datafeed does not use aggregations.
The maximum value is the value of `index.max_result_window`, which is 10,000 by default.
** *`allow_no_indices` (Optional, boolean)*: If true, wildcard indices expressions that resolve into no concrete indices are ignored. This includes the `_all`
string or when no indices are specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target data streams, this argument determines
whether wildcard expressions match hidden data streams. Supports a list of values.
** *`ignore_throttled` (Optional, boolean)*: If true, concrete, expanded, or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If true, unavailable indices (missing or closed) are ignored.

[discrete]
==== put_filter
Instantiates a filter.

{ref}/ml-put-filter.html[Endpoint documentation]
[source,ts]
----
client.ml.putFilter({ filter_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`filter_id` (string)*: A string that uniquely identifies a filter.
** *`description` (Optional, string)*: A description of the filter.
** *`items` (Optional, string[])*: The items of the filter. A wildcard `*` can be used at the beginning or the end of an item.
Up to 10000 items are allowed in each filter.

[discrete]
==== put_job
Instantiates an anomaly detection job.

{ref}/ml-put-job.html[Endpoint documentation]
[source,ts]
----
client.ml.putJob({ job_id, analysis_config, data_description })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: The identifier for the anomaly detection job. This identifier can contain lowercase alphanumeric characters (a-z and 0-9), hyphens, and underscores. It must start and end with alphanumeric characters.
** *`analysis_config` ({ bucket_span, categorization_analyzer, categorization_field_name, categorization_filters, detectors, influencers, latency, model_prune_window, multivariate_by_fields, per_partition_categorization, summary_count_field_name })*: Specifies how to analyze the data. After you create a job, you cannot change the analysis configuration; all the properties are informational.
** *`data_description` ({ format, time_field, time_format, field_delimiter })*: Defines the format of the input data when you send data to the job by using the post data API. Note that when configure a datafeed, these properties are automatically set. When data is received via the post data API, it is not stored in Elasticsearch. Only the results for anomaly detection are retained.
** *`allow_lazy_open` (Optional, boolean)*: Advanced configuration option. Specifies whether this job can open when there is insufficient machine learning node capacity for it to be immediately assigned to a node. By default, if a machine learning node with capacity to run the job cannot immediately be found, the open anomaly detection jobs API returns an error. However, this is also subject to the cluster-wide `xpack.ml.max_lazy_ml_nodes` setting. If this option is set to true, the open anomaly detection jobs API does not return an error and the job waits in the opening state until sufficient machine learning node capacity is available.
** *`analysis_limits` (Optional, { categorization_examples_limit, model_memory_limit })*: Limits can be applied for the resources required to hold the mathematical models in memory. These limits are approximate and can be set per job. They do not control the memory used by other processes, for example the Elasticsearch Java processes.
** *`background_persist_interval` (Optional, string | -1 | 0)*: Advanced configuration option. The time between each periodic persistence of the model. The default value is a randomized value between 3 to 4 hours, which avoids all jobs persisting at exactly the same time. The smallest allowed value is 1 hour. For very large models (several GB), persistence could take 10-20 minutes, so do not set the `background_persist_interval` value too low.
** *`custom_settings` (Optional, User-defined value)*: Advanced configuration option. Contains custom meta data about the job.
** *`daily_model_snapshot_retention_after_days` (Optional, number)*: Advanced configuration option, which affects the automatic removal of old model snapshots for this job. It specifies a period of time (in days) after which only the first snapshot per day is retained. This period is relative to the timestamp of the most recent snapshot for this job. Valid values range from 0 to `model_snapshot_retention_days`.
** *`datafeed_config` (Optional, { aggregations, chunking_config, datafeed_id, delayed_data_check_config, frequency, indices, indices_options, job_id, max_empty_searches, query, query_delay, runtime_mappings, script_fields, scroll_size })*: Defines a datafeed for the anomaly detection job. If Elasticsearch security features are enabled, your datafeed remembers which roles the user who created it had at the time of creation and runs the query using those same roles. If you provide secondary authorization headers, those credentials are used instead.
** *`description` (Optional, string)*: A description of the job.
** *`groups` (Optional, string[])*: A list of job groups. A job can belong to no groups or many.
** *`model_plot_config` (Optional, { enabled })*: This advanced configuration option stores model information along with the results. It provides a more detailed view into anomaly detection. If you enable model plot it can add considerable overhead to the performance of the system; it is not feasible for jobs with many entities. Model plot provides a simplified and indicative view of the model and its bounds. It does not display complex features such as multivariate correlations or multimodal data. As such, anomalies may occasionally be reported which cannot be seen in the model plot. Model plot config can be configured when the job is created or updated later. It must be disabled if performance issues are experienced.
** *`model_snapshot_retention_days` (Optional, number)*: Advanced configuration option, which affects the automatic removal of old model snapshots for this job. It specifies the maximum period of time (in days) that snapshots are retained. This period is relative to the timestamp of the most recent snapshot for this job. By default, snapshots ten days older than the newest snapshot are deleted.
** *`renormalization_window_days` (Optional, number)*: Advanced configuration option. The period over which adjustments to the score are applied, as new data is seen. The default value is the longer of 30 days or 100 bucket spans.
** *`results_index_name` (Optional, string)*: A text string that affects the name of the machine learning results index. By default, the job generates an index named `.ml-anomalies-shared`.
** *`results_retention_days` (Optional, number)*: Advanced configuration option. The period of time (in days) that results are retained. Age is calculated relative to the timestamp of the latest bucket result. If this property has a non-null value, once per day at 00:30 (server time), results that are the specified number of days older than the latest bucket result are deleted from Elasticsearch. The default value is null, which means all results are retained. Annotations generated by the system also count as results for retention purposes; they are deleted after the same number of days as results. Annotations added by users are retained forever.

[discrete]
==== put_trained_model
Creates an inference trained model.

{ref}/put-trained-models.html[Endpoint documentation]
[source,ts]
----
client.ml.putTrainedModel({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`compressed_definition` (Optional, string)*: The compressed (GZipped and Base64 encoded) inference definition of the
model. If compressed_definition is specified, then definition cannot be
specified.
** *`definition` (Optional, { preprocessors, trained_model })*: The inference definition for the model. If definition is specified, then
compressed_definition cannot be specified.
** *`description` (Optional, string)*: A human-readable description of the inference trained model.
** *`inference_config` (Optional, { regression, classification })*: The default configuration for inference. This can be either a regression
or classification configuration. It must match the underlying
definition.trained_model's target_type. For pre-packaged models such as
ELSER the config is not required.
** *`input` (Optional, { field_names })*: The input field names for the model definition.
** *`metadata` (Optional, User-defined value)*: An object map that contains metadata about the model.
** *`model_type` (Optional, Enum("tree_ensemble" | "lang_ident" | "pytorch"))*: The model type.
** *`model_size_bytes` (Optional, number)*: The estimated memory usage in bytes to keep the trained model in memory.
This property is supported only if defer_definition_decompression is true
or the model definition is not supplied.
** *`platform_architecture` (Optional, string)*: The platform architecture (if applicable) of the trained mode. If the model
only works on one platform, because it is heavily optimized for a particular
processor architecture and OS combination, then this field specifies which.
The format of the string must match the platform identifiers used by Elasticsearch,
so one of, `linux-x86_64`, `linux-aarch64`, `darwin-x86_64`, `darwin-aarch64`,
or `windows-x86_64`. For portable models (those that work independent of processor
architecture or OS features), leave this field unset.
** *`tags` (Optional, string[])*: An array of tags to organize the model.

[discrete]
==== put_trained_model_alias
Creates a new model alias (or reassigns an existing one) to refer to the trained model

{ref}/put-trained-models-aliases.html[Endpoint documentation]
[source,ts]
----
client.ml.putTrainedModelAlias({ model_alias, model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_alias` (string)*: The alias to create or update. This value cannot end in numbers.
** *`model_id` (string)*: The identifier for the trained model that the alias refers to.
** *`reassign` (Optional, boolean)*: Specifies whether the alias gets reassigned to the specified trained
model if it is already assigned to a different model. If the alias is
already assigned and this parameter is false, the API returns an error.

[discrete]
==== put_trained_model_definition_part
Creates part of a trained model definition

{ref}/put-trained-model-definition-part.html[Endpoint documentation]
[source,ts]
----
client.ml.putTrainedModelDefinitionPart({ model_id, part, definition, total_definition_length, total_parts })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`part` (number)*: The definition part number. When the definition is loaded for inference the definition parts are streamed in the
order of their part number. The first part must be `0` and the final part must be `total_parts - 1`.
** *`definition` (string)*: The definition part for the model. Must be a base64 encoded string.
** *`total_definition_length` (number)*: The total uncompressed definition length in bytes. Not base64 encoded.
** *`total_parts` (number)*: The total number of parts that will be uploaded. Must be greater than 0.

[discrete]
==== put_trained_model_vocabulary
Creates a trained model vocabulary

{ref}/put-trained-model-vocabulary.html[Endpoint documentation]
[source,ts]
----
client.ml.putTrainedModelVocabulary({ model_id, vocabulary })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`vocabulary` (string[])*: The model vocabulary, which must not be empty.

[discrete]
==== reset_job
Resets an existing anomaly detection job.

{ref}/ml-reset-job.html[Endpoint documentation]
[source,ts]
----
client.ml.resetJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: The ID of the job to reset.
** *`wait_for_completion` (Optional, boolean)*: Should this request wait until the operation has completed before
returning.
** *`delete_user_annotations` (Optional, boolean)*: Specifies whether annotations that have been added by the
user should be deleted along with any auto-generated annotations when the job is
reset.

[discrete]
==== start_data_frame_analytics
Starts a data frame analytics job.

{ref}/start-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.startDataFrameAnalytics({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`timeout` (Optional, string | -1 | 0)*: Controls the amount of time to wait until the data frame analytics job
starts.

[discrete]
==== start_datafeed
Starts one or more datafeeds.

{ref}/ml-start-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.startDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: A numerical character string that uniquely identifies the datafeed. This identifier can contain lowercase
alphanumeric characters (a-z and 0-9), hyphens, and underscores. It must start and end with alphanumeric
characters.
** *`end` (Optional, string | Unit)*: Refer to the description for the `end` query parameter.
** *`start` (Optional, string | Unit)*: Refer to the description for the `start` query parameter.
** *`timeout` (Optional, string | -1 | 0)*: Refer to the description for the `timeout` query parameter.

[discrete]
==== start_trained_model_deployment
Start a trained model deployment.

{ref}/start-trained-model-deployment.html[Endpoint documentation]
[source,ts]
----
client.ml.startTrainedModelDeployment({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model. Currently, only PyTorch models are supported.
** *`cache_size` (Optional, number | string)*: The inference cache size (in memory outside the JVM heap) per node for the model.
The default value is the same size as the `model_size_bytes`. To disable the cache,
`0b` can be provided.
** *`number_of_allocations` (Optional, number)*: The number of model allocations on each node where the model is deployed.
All allocations on a node share the same copy of the model in memory but use
a separate set of threads to evaluate the model.
Increasing this value generally increases the throughput.
If this setting is greater than the number of hardware threads
it will automatically be changed to a value less than the number of hardware threads.
** *`priority` (Optional, Enum("normal" | "low"))*: The deployment priority.
** *`queue_capacity` (Optional, number)*: Specifies the number of inference requests that are allowed in the queue. After the number of requests exceeds
this value, new requests are rejected with a 429 error.
** *`threads_per_allocation` (Optional, number)*: Sets the number of threads used by each model allocation during inference. This generally increases
the inference speed. The inference process is a compute-bound process; any number
greater than the number of available hardware threads on the machine does not increase the
inference speed. If this setting is greater than the number of hardware threads
it will automatically be changed to a value less than the number of hardware threads.
** *`timeout` (Optional, string | -1 | 0)*: Specifies the amount of time to wait for the model to deploy.
** *`wait_for` (Optional, Enum("started" | "starting" | "fully_allocated"))*: Specifies the allocation status to wait for before returning.

[discrete]
==== stop_data_frame_analytics
Stops one or more data frame analytics jobs.

{ref}/stop-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.stopDataFrameAnalytics({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no data frame analytics
jobs that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

The default value is true, which returns an empty data_frame_analytics
array when there are no matches and the subset of results when there are
partial matches. If this parameter is false, the request returns a 404
status code when there are no matches or only partial matches.
** *`force` (Optional, boolean)*: If true, the data frame analytics job is stopped forcefully.
** *`timeout` (Optional, string | -1 | 0)*: Controls the amount of time to wait until the data frame analytics job
stops. Defaults to 20 seconds.

[discrete]
==== stop_datafeed
Stops one or more datafeeds.

{ref}/ml-stop-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.stopDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: Identifier for the datafeed. You can stop multiple datafeeds in a single API request by using a comma-separated
list of datafeeds or a wildcard expression. You can close all datafeeds by using `_all` or by specifying `*` as
the identifier.
** *`allow_no_match` (Optional, boolean)*: Refer to the description for the `allow_no_match` query parameter.
** *`force` (Optional, boolean)*: Refer to the description for the `force` query parameter.
** *`timeout` (Optional, string | -1 | 0)*: Refer to the description for the `timeout` query parameter.

[discrete]
==== stop_trained_model_deployment
Stop a trained model deployment.

{ref}/stop-trained-model-deployment.html[Endpoint documentation]
[source,ts]
----
client.ml.stopTrainedModelDeployment({ model_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`model_id` (string)*: The unique identifier of the trained model.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request: contains wildcard expressions and there are no deployments that match;
contains the  `_all` string or no identifiers and there are no matches; or contains wildcard expressions and
there are only partial matches. By default, it returns an empty array when there are no matches and the subset of results when there are partial matches.
If `false`, the request returns a 404 status code when there are no matches or only partial matches.
** *`force` (Optional, boolean)*: Forcefully stops the deployment, even if it is used by ingest pipelines. You can't use these pipelines until you
restart the model deployment.

[discrete]
==== update_data_frame_analytics
Updates certain properties of a data frame analytics job.

{ref}/update-dfanalytics.html[Endpoint documentation]
[source,ts]
----
client.ml.updateDataFrameAnalytics({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the data frame analytics job. This identifier can contain
lowercase alphanumeric characters (a-z and 0-9), hyphens, and
underscores. It must start and end with alphanumeric characters.
** *`description` (Optional, string)*: A description of the job.
** *`model_memory_limit` (Optional, string)*: The approximate maximum amount of memory resources that are permitted for
analytical processing. If your `elasticsearch.yml` file contains an
`xpack.ml.max_model_memory_limit` setting, an error occurs when you try
to create data frame analytics jobs that have `model_memory_limit` values
greater than that setting.
** *`max_num_threads` (Optional, number)*: The maximum number of threads to be used by the analysis. Using more
threads may decrease the time necessary to complete the analysis at the
cost of using more CPU. Note that the process may use additional threads
for operational functionality other than the analysis itself.
** *`allow_lazy_start` (Optional, boolean)*: Specifies whether this job can start when there is insufficient machine
learning node capacity for it to be immediately assigned to a node.

[discrete]
==== update_datafeed
Updates certain properties of a datafeed.

{ref}/ml-update-datafeed.html[Endpoint documentation]
[source,ts]
----
client.ml.updateDatafeed({ datafeed_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`datafeed_id` (string)*: A numerical character string that uniquely identifies the datafeed.
This identifier can contain lowercase alphanumeric characters (a-z and 0-9), hyphens, and underscores.
It must start and end with alphanumeric characters.
** *`aggregations` (Optional, Record<string, { aggregations, meta, adjacency_matrix, auto_date_histogram, avg, avg_bucket, boxplot, bucket_script, bucket_selector, bucket_sort, cardinality, children, composite, cumulative_cardinality, cumulative_sum, date_histogram, date_range, derivative, diversified_sampler, extended_stats, extended_stats_bucket, frequent_item_sets, filter, filters, geo_bounds, geo_centroid, geo_distance, geohash_grid, geo_line, geotile_grid, geohex_grid, global, histogram, ip_range, ip_prefix, inference, line, matrix_stats, max, max_bucket, median_absolute_deviation, min, min_bucket, missing, moving_avg, moving_percentiles, moving_fn, multi_terms, nested, normalize, parent, percentile_ranks, percentiles, percentiles_bucket, range, rare_terms, rate, reverse_nested, sampler, scripted_metric, serial_diff, significant_terms, significant_text, stats, stats_bucket, string_stats, sum, sum_bucket, terms, top_hits, t_test, top_metrics, value_count, weighted_avg, variable_width_histogram }>)*: If set, the datafeed performs aggregation searches. Support for aggregations is limited and should be used only
with low cardinality data.
** *`chunking_config` (Optional, { mode, time_span })*: Datafeeds might search over long time periods, for several months or years. This search is split into time
chunks in order to ensure the load on Elasticsearch is managed. Chunking configuration controls how the size of
these time chunks are calculated; it is an advanced configuration option.
** *`delayed_data_check_config` (Optional, { check_window, enabled })*: Specifies whether the datafeed checks for missing data and the size of the window. The datafeed can optionally
search over indices that have already been read in an effort to determine whether any data has subsequently been
added to the index. If missing data is found, it is a good indication that the `query_delay` is set too low and
the data is being indexed after the datafeed has passed that moment in time. This check runs only on real-time
datafeeds.
** *`frequency` (Optional, string | -1 | 0)*: The interval at which scheduled queries are made while the datafeed runs in real time. The default value is
either the bucket span for short bucket spans, or, for longer bucket spans, a sensible fraction of the bucket
span. When `frequency` is shorter than the bucket span, interim results for the last (partial) bucket are
written then eventually overwritten by the full bucket results. If the datafeed uses aggregations, this value
must be divisible by the interval of the date histogram aggregation.
** *`indices` (Optional, string[])*: An array of index names. Wildcards are supported. If any of the indices are in remote clusters, the machine
learning nodes must have the `remote_cluster_client` role.
** *`indices_options` (Optional, { allow_no_indices, expand_wildcards, ignore_unavailable, ignore_throttled })*: Specifies index expansion options that are used during search.
** *`job_id` (Optional, string)*
** *`max_empty_searches` (Optional, number)*: If a real-time datafeed has never seen any data (including during any initial training period), it automatically
stops and closes the associated job after this many real-time searches return no documents. In other words,
it stops after `frequency` times `max_empty_searches` of real-time operation. If not set, a datafeed with no
end time that sees no data remains started until it is explicitly stopped. By default, it is not set.
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: The Elasticsearch query domain-specific language (DSL). This value corresponds to the query object in an
Elasticsearch search POST body. All the options that are supported by Elasticsearch can be used, as this
object is passed verbatim to Elasticsearch. Note that if you change the query, the analyzed data is also
changed. Therefore, the time required to learn might be long and the understandability of the results is
unpredictable. If you want to make significant changes to the source data, it is recommended that you
clone the job and datafeed and make the amendments in the clone. Let both run in parallel and close one
when you are satisfied with the results of the job.
** *`query_delay` (Optional, string | -1 | 0)*: The number of seconds behind real time that data is queried. For example, if data from 10:04 a.m. might
not be searchable in Elasticsearch until 10:06 a.m., set this property to 120 seconds. The default
value is randomly selected between `60s` and `120s`. This randomness improves the query performance
when there are multiple jobs running on the same node.
** *`runtime_mappings` (Optional, Record<string, { fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Specifies runtime fields for the datafeed search.
** *`script_fields` (Optional, Record<string, { script, ignore_failure }>)*: Specifies scripts that evaluate custom expressions and returns script fields to the datafeed.
The detector configuration objects in a job can contain functions that use these script fields.
** *`scroll_size` (Optional, number)*: The size parameter that is used in Elasticsearch searches when the datafeed does not use aggregations.
The maximum value is the value of `index.max_result_window`.
** *`allow_no_indices` (Optional, boolean)*: If `true`, wildcard indices expressions that resolve into no concrete indices are ignored. This includes the
`_all` string or when no indices are specified.
** *`expand_wildcards` (Optional, Enum("all" | "open" | "closed" | "hidden" | "none") | Enum("all" | "open" | "closed" | "hidden" | "none")[])*: Type of index that wildcard patterns can match. If the request can target data streams, this argument determines
whether wildcard expressions match hidden data streams. Supports a list of values. Valid values are:

* `all`: Match any data stream or index, including hidden ones.
* `closed`: Match closed, non-hidden indices. Also matches any non-hidden data stream. Data streams cannot be closed.
* `hidden`: Match hidden data streams and hidden indices. Must be combined with `open`, `closed`, or both.
* `none`: Wildcard patterns are not accepted.
* `open`: Match open, non-hidden indices. Also matches any non-hidden data stream.
** *`ignore_throttled` (Optional, boolean)*: If `true`, concrete, expanded or aliased indices are ignored when frozen.
** *`ignore_unavailable` (Optional, boolean)*: If `true`, unavailable indices (missing or closed) are ignored.

[discrete]
==== update_filter
Updates the description of a filter, adds items, or removes items.

{ref}/ml-update-filter.html[Endpoint documentation]
[source,ts]
----
client.ml.updateFilter({ filter_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`filter_id` (string)*: A string that uniquely identifies a filter.
** *`add_items` (Optional, string[])*: The items to add to the filter.
** *`description` (Optional, string)*: A description for the filter.
** *`remove_items` (Optional, string[])*: The items to remove from the filter.

[discrete]
==== update_job
Updates certain properties of an anomaly detection job.

{ref}/ml-update-job.html[Endpoint documentation]
[source,ts]
----
client.ml.updateJob({ job_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`job_id` (string)*: Identifier for the job.
** *`allow_lazy_open` (Optional, boolean)*: Advanced configuration option. Specifies whether this job can open when
there is insufficient machine learning node capacity for it to be
immediately assigned to a node. If `false` and a machine learning node
with capacity to run the job cannot immediately be found, the open
anomaly detection jobs API returns an error. However, this is also
subject to the cluster-wide `xpack.ml.max_lazy_ml_nodes` setting. If this
option is set to `true`, the open anomaly detection jobs API does not
return an error and the job waits in the opening state until sufficient
machine learning node capacity is available.
** *`analysis_limits` (Optional, { model_memory_limit })*
** *`background_persist_interval` (Optional, string | -1 | 0)*: Advanced configuration option. The time between each periodic persistence
of the model.
The default value is a randomized value between 3 to 4 hours, which
avoids all jobs persisting at exactly the same time. The smallest allowed
value is 1 hour.
For very large models (several GB), persistence could take 10-20 minutes,
so do not set the value too low.
If the job is open when you make the update, you must stop the datafeed,
close the job, then reopen the job and restart the datafeed for the
changes to take effect.
** *`custom_settings` (Optional, Record<string, User-defined value>)*: Advanced configuration option. Contains custom meta data about the job.
For example, it can contain custom URL information as shown in Adding
custom URLs to machine learning results.
** *`categorization_filters` (Optional, string[])*
** *`description` (Optional, string)*: A description of the job.
** *`model_plot_config` (Optional, { enabled })*
** *`model_prune_window` (Optional, string | -1 | 0)*
** *`daily_model_snapshot_retention_after_days` (Optional, number)*: Advanced configuration option, which affects the automatic removal of old
model snapshots for this job. It specifies a period of time (in days)
after which only the first snapshot per day is retained. This period is
relative to the timestamp of the most recent snapshot for this job. Valid
values range from 0 to `model_snapshot_retention_days`. For jobs created
before version 7.8.0, the default value matches
`model_snapshot_retention_days`.
** *`model_snapshot_retention_days` (Optional, number)*: Advanced configuration option, which affects the automatic removal of old
model snapshots for this job. It specifies the maximum period of time (in
days) that snapshots are retained. This period is relative to the
timestamp of the most recent snapshot for this job.
** *`renormalization_window_days` (Optional, number)*: Advanced configuration option. The period over which adjustments to the
score are applied, as new data is seen.
** *`results_retention_days` (Optional, number)*: Advanced configuration option. The period of time (in days) that results
are retained. Age is calculated relative to the timestamp of the latest
bucket result. If this property has a non-null value, once per day at
00:30 (server time), results that are the specified number of days older
than the latest bucket result are deleted from Elasticsearch. The default
value is null, which means all results are retained.
** *`groups` (Optional, string[])*: A list of job groups. A job can belong to no groups or many.
** *`detectors` (Optional, { by_field_name, custom_rules, detector_description, detector_index, exclude_frequent, field_name, function, over_field_name, partition_field_name, use_null }[])*: An array of detector update objects.
** *`per_partition_categorization` (Optional, { enabled, stop_on_warn })*: Settings related to how categorization interacts with partition fields.

[discrete]
=== query_ruleset
[discrete]
==== delete
Deletes a query ruleset.

{ref}/delete-query-ruleset.html[Endpoint documentation]
[source,ts]
----
client.queryRuleset.delete({ ruleset_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset to delete

[discrete]
==== get
Returns the details about a query ruleset.

{ref}/get-query-ruleset.html[Endpoint documentation]
[source,ts]
----
client.queryRuleset.get({ ruleset_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset

[discrete]
==== list
Lists query rulesets.

{ref}/list-query-rulesets.html[Endpoint documentation]
[source,ts]
----
client.queryRuleset.list({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`from` (Optional, number)*: Starting offset (default: 0)
** *`size` (Optional, number)*: specifies a max number of results to get

[discrete]
==== put
Creates or updates a query ruleset.

{ref}/put-query-ruleset.html[Endpoint documentation]
[source,ts]
----
client.queryRuleset.put({ ruleset_id, rules })
----

[discrete]
==== Arguments

* *Request (object):*
** *`ruleset_id` (string)*: The unique identifier of the query ruleset to be created or updated
** *`rules` ({ rule_id, type, criteria, actions }[])*

[discrete]
=== search_application
[discrete]
==== delete
Deletes a search application.

{ref}/put-search-application.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.delete({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the search application to delete

[discrete]
==== delete_behavioral_analytics
Delete a behavioral analytics collection.

{ref}/delete-analytics-collection.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.deleteBehavioralAnalytics({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the analytics collection to be deleted

[discrete]
==== get
Returns the details about a search application.

{ref}/get-search-application.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.get({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the search application

[discrete]
==== get_behavioral_analytics
Returns the existing behavioral analytics collections.

{ref}/list-analytics-collection.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.getBehavioralAnalytics({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (Optional, string[])*: A list of analytics collections to limit the returned information

[discrete]
==== list
Returns the existing search applications.

{ref}/list-search-applications.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.list({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`q` (Optional, string)*: Query in the Lucene query string syntax.
** *`from` (Optional, number)*: Starting offset.
** *`size` (Optional, number)*: Specifies a max number of results to get.

[discrete]
==== put
Creates or updates a search application.

{ref}/put-search-application.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.put({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the search application to be created or updated.
** *`search_application` (Optional, { name, indices, updated_at_millis, analytics_collection_name, template })*
** *`create` (Optional, boolean)*: If `true`, this request cannot replace or update existing Search Applications.

[discrete]
==== put_behavioral_analytics
Creates a behavioral analytics collection.

{ref}/put-analytics-collection.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.putBehavioralAnalytics({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the analytics collection to be created or updated.

[discrete]
==== search
Perform a search against a search application

{ref}/search-application-search.html[Endpoint documentation]
[source,ts]
----
client.searchApplication.search({ name })
----

[discrete]
==== Arguments

* *Request (object):*
** *`name` (string)*: The name of the search application to be searched.
** *`params` (Optional, Record<string, User-defined value>)*: Query parameters specific to this request, which will override any defaults specified in the template.

[discrete]
=== security
[discrete]
==== authenticate
Enables authentication as a user and retrieve information about the authenticated user.

{ref}/security-api-authenticate.html[Endpoint documentation]
[source,ts]
----
client.security.authenticate()
----


[discrete]
==== create_api_key
Creates an API key for access without requiring basic authentication.

{ref}/security-api-create-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.createApiKey({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`expiration` (Optional, string | -1 | 0)*: Expiration time for the API key. By default, API keys never expire.
** *`name` (Optional, string)*: Specifies the name for this API key.
** *`role_descriptors` (Optional, Record<string, { cluster, indices, global, applications, metadata, run_as, transient_metadata }>)*: An array of role descriptors for this API key. This parameter is optional. When it is not specified or is an empty array, then the API key will have a point in time snapshot of permissions of the authenticated user. If you supply role descriptors then the resultant permissions would be an intersection of API keys permissions and authenticated user’s permissions thereby limiting the access scope for API keys. The structure of role descriptor is the same as the request for create role API. For more details, see create or update roles API.
** *`refresh` (Optional, Enum(true | false | "wait_for"))*: If `true` (the default) then refresh the affected shards to make this operation visible to search, if `wait_for` then wait for a refresh to make this operation visible to search, if `false` then do nothing with refreshes.

[discrete]
==== get_api_key
Retrieves information for one or more API keys.

{ref}/security-api-get-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.getApiKey({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*: An API key id.
This parameter cannot be used with any of `name`, `realm_name` or `username`.
** *`name` (Optional, string)*: An API key name.
This parameter cannot be used with any of `id`, `realm_name` or `username`.
It supports prefix search with wildcard.
** *`owner` (Optional, boolean)*: A boolean flag that can be used to query API keys owned by the currently authenticated user.
The `realm_name` or `username` parameters cannot be specified when this parameter is set to `true` as they are assumed to be the currently authenticated ones.
** *`realm_name` (Optional, string)*: The name of an authentication realm.
This parameter cannot be used with either `id` or `name` or when `owner` flag is set to `true`.
** *`username` (Optional, string)*: The username of a user.
This parameter cannot be used with either `id` or `name` or when `owner` flag is set to `true`.

[discrete]
==== has_privileges
Determines whether the specified user has a specified list of privileges.

{ref}/security-api-has-privileges.html[Endpoint documentation]
[source,ts]
----
client.security.hasPrivileges({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`user` (Optional, string)*: Username
** *`application` (Optional, { application, privileges, resources }[])*
** *`cluster` (Optional, Enum("all" | "cancel_task" | "create_snapshot" | "grant_api_key" | "manage" | "manage_api_key" | "manage_ccr" | "manage_enrich" | "manage_ilm" | "manage_index_templates" | "manage_ingest_pipelines" | "manage_logstash_pipelines" | "manage_ml" | "manage_oidc" | "manage_own_api_key" | "manage_pipeline" | "manage_rollup" | "manage_saml" | "manage_security" | "manage_service_account" | "manage_slm" | "manage_token" | "manage_transform" | "manage_user_profile" | "manage_watcher" | "monitor" | "monitor_ml" | "monitor_rollup" | "monitor_snapshot" | "monitor_text_structure" | "monitor_transform" | "monitor_watcher" | "read_ccr" | "read_ilm" | "read_pipeline" | "read_slm" | "transport_client")[])*: A list of the cluster privileges that you want to check.
** *`index` (Optional, { names, privileges, allow_restricted_indices }[])*

[discrete]
==== invalidate_api_key
Invalidates one or more API keys.

{ref}/security-api-invalidate-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.invalidateApiKey({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (Optional, string)*
** *`ids` (Optional, string[])*: A list of API key ids.
This parameter cannot be used with any of `name`, `realm_name`, or `username`.
** *`name` (Optional, string)*: An API key name.
This parameter cannot be used with any of `ids`, `realm_name` or `username`.
** *`owner` (Optional, boolean)*: Can be used to query API keys owned by the currently authenticated user.
The `realm_name` or `username` parameters cannot be specified when this parameter is set to `true` as they are assumed to be the currently authenticated ones.
** *`realm_name` (Optional, string)*: The name of an authentication realm.
This parameter cannot be used with either `ids` or `name`, or when `owner` flag is set to `true`.
** *`username` (Optional, string)*: The username of a user.
This parameter cannot be used with either `ids` or `name`, or when `owner` flag is set to `true`.

[discrete]
==== query_api_keys
Retrieves information for API keys using a subset of query DSL

{ref}/security-api-query-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.queryApiKeys({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`query` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: A query to filter which API keys to return.
The query supports a subset of query types, including `match_all`, `bool`, `term`, `terms`, `ids`, `prefix`, `wildcard`, and `range`.
You can query all public information associated with an API key.
** *`from` (Optional, number)*: Starting document offset.
By default, you cannot page through more than 10,000 hits using the from and size parameters.
To page through more hits, use the `search_after` parameter.
** *`sort` (Optional, string | { _score, _doc, _geo_distance, _script } | string | { _score, _doc, _geo_distance, _script }[])*: Other than `id`, all public fields of an API key are eligible for sorting.
In addition, sort can also be applied to the `_doc` field to sort by index order.
** *`size` (Optional, number)*: The number of hits to return.
By default, you cannot page through more than 10,000 hits using the `from` and `size` parameters.
To page through more hits, use the `search_after` parameter.
** *`search_after` (Optional, number | number | string | boolean | null | User-defined value[])*: Search after definition

[discrete]
==== update_api_key
Updates attributes of an existing API key.

{ref}/security-api-update-api-key.html[Endpoint documentation]
[source,ts]
----
client.security.updateApiKey({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The ID of the API key to update.
** *`role_descriptors` (Optional, Record<string, { cluster, indices, global, applications, metadata, run_as, transient_metadata }>)*: An array of role descriptors for this API key. This parameter is optional. When it is not specified or is an empty array, then the API key will have a point in time snapshot of permissions of the authenticated user. If you supply role descriptors then the resultant permissions would be an intersection of API keys permissions and authenticated user’s permissions thereby limiting the access scope for API keys. The structure of role descriptor is the same as the request for create role API. For more details, see create or update roles API.
** *`metadata` (Optional, Record<string, User-defined value>)*: Arbitrary metadata that you want to associate with the API key. It supports nested data structure. Within the metadata object, keys beginning with _ are reserved for system usage.

[discrete]
=== sql
[discrete]
==== clear_cursor
Clears the SQL cursor

{ref}/clear-sql-cursor-api.html[Endpoint documentation]
[source,ts]
----
client.sql.clearCursor({ cursor })
----

[discrete]
==== Arguments

* *Request (object):*
** *`cursor` (string)*: Cursor to clear.

[discrete]
==== delete_async
Deletes an async SQL search or a stored synchronous SQL search. If the search is still running, the API cancels it.

{ref}/delete-async-sql-search-api.html[Endpoint documentation]
[source,ts]
----
client.sql.deleteAsync({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.

[discrete]
==== get_async
Returns the current status and available results for an async SQL search or stored synchronous SQL search

{ref}/get-async-sql-search-api.html[Endpoint documentation]
[source,ts]
----
client.sql.getAsync({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.
** *`delimiter` (Optional, string)*: Separator for CSV results. The API only supports this parameter for CSV responses.
** *`format` (Optional, string)*: Format for the response. You must specify a format using this parameter or the
Accept HTTP header. If you specify both, the API uses this parameter.
** *`keep_alive` (Optional, string | -1 | 0)*: Retention period for the search and its results. Defaults
to the `keep_alive` period for the original SQL search.
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Period to wait for complete results. Defaults to no timeout,
meaning the request waits for complete search results.

[discrete]
==== get_async_status
Returns the current status of an async SQL search or a stored synchronous SQL search

{ref}/get-async-sql-search-status-api.html[Endpoint documentation]
[source,ts]
----
client.sql.getAsyncStatus({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: Identifier for the search.

[discrete]
==== query
Executes a SQL request

{ref}/sql-search-api.html[Endpoint documentation]
[source,ts]
----
client.sql.query({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`catalog` (Optional, string)*: Default catalog (cluster) for queries. If unspecified, the queries execute on the data in the local cluster only.
** *`columnar` (Optional, boolean)*: If true, the results in a columnar fashion: one row represents all the values of a certain column from the current page of results.
** *`cursor` (Optional, string)*: Cursor used to retrieve a set of paginated results.
If you specify a cursor, the API only uses the `columnar` and `time_zone` request body parameters.
It ignores other request body parameters.
** *`fetch_size` (Optional, number)*: The maximum number of rows (or entries) to return in one response
** *`filter` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Elasticsearch query DSL for additional filtering.
** *`query` (Optional, string)*: SQL query to run.
** *`request_timeout` (Optional, string | -1 | 0)*: The timeout before the request fails.
** *`page_timeout` (Optional, string | -1 | 0)*: The timeout before a pagination request fails.
** *`time_zone` (Optional, string)*: ISO-8601 time zone ID for the search.
** *`field_multi_value_leniency` (Optional, boolean)*: Throw an exception when encountering multiple values for a field (default) or be lenient and return the first value from the list (without any guarantees of what that will be - typically the first in natural ascending order).
** *`runtime_mappings` (Optional, Record<string, { fetch_fields, format, input_field, target_field, target_index, script, type }>)*: Defines one or more runtime fields in the search request. These fields take
precedence over mapped fields with the same name.
** *`wait_for_completion_timeout` (Optional, string | -1 | 0)*: Period to wait for complete results. Defaults to no timeout, meaning the request waits for complete search results. If the search doesn’t finish within this period, the search becomes async.
** *`params` (Optional, Record<string, User-defined value>)*: Values for parameters in the query.
** *`keep_alive` (Optional, string | -1 | 0)*: Retention period for an async or saved synchronous search.
** *`keep_on_completion` (Optional, boolean)*: If true, Elasticsearch stores synchronous searches if you also specify the wait_for_completion_timeout parameter. If false, Elasticsearch only stores async searches that don’t finish before the wait_for_completion_timeout.
** *`index_using_frozen` (Optional, boolean)*: If true, the search can run on frozen indices. Defaults to false.
** *`format` (Optional, string)*: Format for the response.

[discrete]
==== translate
Translates SQL into Elasticsearch queries

{ref}/sql-translate-api.html[Endpoint documentation]
[source,ts]
----
client.sql.translate({ query })
----

[discrete]
==== Arguments

* *Request (object):*
** *`query` (string)*: SQL query to run.
** *`fetch_size` (Optional, number)*: The maximum number of rows (or entries) to return in one response.
** *`filter` (Optional, { bool, boosting, common, constant_score, dis_max, distance_feature, exists, function_score, fuzzy, geo_bounding_box, geo_distance, geo_polygon, geo_shape, has_child, has_parent, ids, intervals, match, match_all, match_bool_prefix, match_none, match_phrase, match_phrase_prefix, more_like_this, multi_match, nested, parent_id, percolate, pinned, prefix, query_string, range, rank_feature, regexp, rule_query, script, script_score, shape, simple_query_string, span_containing, field_masking_span, span_first, span_multi, span_near, span_not, span_or, span_term, span_within, term, terms, terms_set, wildcard, wrapper, type })*: Elasticsearch query DSL for additional filtering.
** *`time_zone` (Optional, string)*: ISO-8601 time zone ID for the search.

[discrete]
=== synonyms
[discrete]
==== delete_synonym
Deletes a synonym set

{ref}/delete-synonyms-set.html[Endpoint documentation]
[source,ts]
----
client.synonyms.deleteSynonym({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The id of the synonyms set to be deleted

[discrete]
==== delete_synonym_rule
Deletes a synonym rule in a synonym set

{ref}/delete-synonym-rule.html[Endpoint documentation]
[source,ts]
----
client.synonyms.deleteSynonymRule({ set_id, rule_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`set_id` (string)*: The id of the synonym set to be updated
** *`rule_id` (string)*: The id of the synonym rule to be deleted

[discrete]
==== get_synonym
Retrieves a synonym set

{ref}/get-synonyms-set.html[Endpoint documentation]
[source,ts]
----
client.synonyms.getSynonym({ id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: "The id of the synonyms set to be retrieved
** *`from` (Optional, number)*: Starting offset for query rules to be retrieved
** *`size` (Optional, number)*: specifies a max number of query rules to retrieve

[discrete]
==== get_synonym_rule
Retrieves a synonym rule from a synonym set

{ref}/get-synonym-rule.html[Endpoint documentation]
[source,ts]
----
client.synonyms.getSynonymRule({ set_id, rule_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`set_id` (string)*: The id of the synonym set to retrieve the synonym rule from
** *`rule_id` (string)*: The id of the synonym rule to retrieve

[discrete]
==== get_synonyms_sets
Retrieves a summary of all defined synonym sets

{ref}/list-synonyms-sets.html[Endpoint documentation]
[source,ts]
----
client.synonyms.getSynonymsSets({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`from` (Optional, number)*: Starting offset
** *`size` (Optional, number)*: specifies a max number of results to get

[discrete]
==== put_synonym
Creates or updates a synonyms set

{ref}/put-synonyms-set.html[Endpoint documentation]
[source,ts]
----
client.synonyms.putSynonym({ id, synonyms_set })
----

[discrete]
==== Arguments

* *Request (object):*
** *`id` (string)*: The id of the synonyms set to be created or updated
** *`synonyms_set` ({ id, synonyms }[])*: The synonym set information to update

[discrete]
==== put_synonym_rule
Creates or updates a synonym rule in a synonym set

{ref}/put-synonym-rule.html[Endpoint documentation]
[source,ts]
----
client.synonyms.putSynonymRule({ set_id, rule_id, synonyms })
----

[discrete]
==== Arguments

* *Request (object):*
** *`set_id` (string)*: The id of the synonym set to be updated with the synonym rule
** *`rule_id` (string)*: The id of the synonym rule to be updated or created
** *`synonyms` (string[])*

[discrete]
=== tasks
[discrete]
==== get
Returns information about a task.

{ref}/tasks.html[Endpoint documentation]
[source,ts]
----
client.tasks.get({ task_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`task_id` (string)*: ID of the task.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response.
If no response is received before the timeout expires, the request fails and returns an error.
** *`wait_for_completion` (Optional, boolean)*: If `true`, the request blocks until the task has completed.

[discrete]
=== transform
[discrete]
==== delete_transform
Deletes an existing transform.

{ref}/delete-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.deleteTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform.
** *`force` (Optional, boolean)*: If this value is false, the transform must be stopped before it can be deleted. If true, the transform is
deleted regardless of its current state.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== get_transform
Retrieves configuration information for transforms.

{ref}/get-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.getTransform({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (Optional, string | string[])*: Identifier for the transform. It can be a transform identifier or a
wildcard expression. You can get information for all transforms by using
`_all`, by specifying `*` as the `<transform_id>`, or by omitting the
`<transform_id>`.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no transforms that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

If this parameter is false, the request returns a 404 status code when
there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of transforms.
** *`size` (Optional, number)*: Specifies the maximum number of transforms to obtain.
** *`exclude_generated` (Optional, boolean)*: Excludes fields that were automatically added when creating the
transform. This allows the configuration to be in an acceptable format to
be retrieved and then added to another cluster.

[discrete]
==== get_transform_stats
Retrieves usage information for transforms.

{ref}/get-transform-stats.html[Endpoint documentation]
[source,ts]
----
client.transform.getTransformStats({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string | string[])*: Identifier for the transform. It can be a transform identifier or a
wildcard expression. You can get information for all transforms by using
`_all`, by specifying `*` as the `<transform_id>`, or by omitting the
`<transform_id>`.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request:

1. Contains wildcard expressions and there are no transforms that match.
2. Contains the _all string or no identifiers and there are no matches.
3. Contains wildcard expressions and there are only partial matches.

If this parameter is false, the request returns a 404 status code when
there are no matches or only partial matches.
** *`from` (Optional, number)*: Skips the specified number of transforms.
** *`size` (Optional, number)*: Specifies the maximum number of transforms to obtain.
** *`timeout` (Optional, string | -1 | 0)*: Controls the time to wait for the stats

[discrete]
==== preview_transform
Previews a transform.

{ref}/preview-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.previewTransform({ ... })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (Optional, string)*: Identifier for the transform to preview. If you specify this path parameter, you cannot provide transform
configuration details in the request body.
** *`dest` (Optional, { index, op_type, pipeline, routing, version_type })*: The destination for the transform.
** *`description` (Optional, string)*: Free text description of the transform.
** *`frequency` (Optional, string | -1 | 0)*: The interval between checks for changes in the source indices when the
transform is running continuously. Also determines the retry interval in
the event of transient failures while the transform is searching or
indexing. The minimum value is 1s and the maximum is 1h.
** *`pivot` (Optional, { aggregations, group_by })*: The pivot method transforms the data by aggregating and grouping it.
These objects define the group by fields and the aggregation to reduce
the data.
** *`source` (Optional, { index, query, remote, size, slice, sort, _source, runtime_mappings })*: The source of the data for the transform.
** *`settings` (Optional, { align_checkpoints, dates_as_epoch_millis, deduce_mappings, docs_per_second, max_page_search_size })*: Defines optional transform settings.
** *`sync` (Optional, { time })*: Defines the properties transforms require to run continuously.
** *`retention_policy` (Optional, { time })*: Defines a retention policy for the transform. Data that meets the defined
criteria is deleted from the destination index.
** *`latest` (Optional, { sort, unique_key })*: The latest method transforms the data by finding the latest document for
each unique key.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the
timeout expires, the request fails and returns an error.

[discrete]
==== put_transform
Instantiates a transform.

{ref}/put-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.putTransform({ transform_id, dest, source })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform. This identifier can contain lowercase alphanumeric characters (a-z and 0-9),
hyphens, and underscores. It has a 64 character limit and must start and end with alphanumeric characters.
** *`dest` ({ index, op_type, pipeline, routing, version_type })*: The destination for the transform.
** *`source` ({ index, query, remote, size, slice, sort, _source, runtime_mappings })*: The source of the data for the transform.
** *`description` (Optional, string)*: Free text description of the transform.
** *`frequency` (Optional, string | -1 | 0)*: The interval between checks for changes in the source indices when the transform is running continuously. Also
determines the retry interval in the event of transient failures while the transform is searching or indexing.
The minimum value is `1s` and the maximum is `1h`.
** *`latest` (Optional, { sort, unique_key })*: The latest method transforms the data by finding the latest document for each unique key.
** *`_meta` (Optional, Record<string, User-defined value>)*: Defines optional transform metadata.
** *`pivot` (Optional, { aggregations, group_by })*: The pivot method transforms the data by aggregating and grouping it. These objects define the group by fields
and the aggregation to reduce the data.
** *`retention_policy` (Optional, { time })*: Defines a retention policy for the transform. Data that meets the defined criteria is deleted from the
destination index.
** *`settings` (Optional, { align_checkpoints, dates_as_epoch_millis, deduce_mappings, docs_per_second, max_page_search_size })*: Defines optional transform settings.
** *`sync` (Optional, { time })*: Defines the properties transforms require to run continuously.
** *`defer_validation` (Optional, boolean)*: When the transform is created, a series of validations occur to ensure its success. For example, there is a
check for the existence of the source indices and a check that the destination index is not part of the source
index pattern. You can use this parameter to skip the checks, for example when the source index does not exist
until after the transform is created. The validations are always run when you start the transform, however, with
the exception of privilege checks.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.

[discrete]
==== reset_transform
Resets an existing transform.

{ref}/reset-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.resetTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform. This identifier can contain lowercase alphanumeric characters (a-z and 0-9),
hyphens, and underscores. It has a 64 character limit and must start and end with alphanumeric characters.
** *`force` (Optional, boolean)*: If this value is `true`, the transform is reset regardless of its current state. If it's `false`, the transform
must be stopped before it can be reset.

[discrete]
==== schedule_now_transform
Schedules now a transform.

{ref}/schedule-now-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.scheduleNowTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform.
** *`timeout` (Optional, string | -1 | 0)*: Controls the time to wait for the scheduling to take place

[discrete]
==== start_transform
Starts one or more transforms.

{ref}/start-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.startTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the timeout expires, the request fails and returns an error.
** *`from` (Optional, string)*: Restricts the set of transformed entities to those changed after this time. Relative times like now-30d are supported. Only applicable for continuous transforms.

[discrete]
==== stop_transform
Stops one or more transforms.

{ref}/stop-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.stopTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform. To stop multiple transforms, use a list or a wildcard expression.
To stop all transforms, use `_all` or `*` as the identifier.
** *`allow_no_match` (Optional, boolean)*: Specifies what to do when the request: contains wildcard expressions and there are no transforms that match;
contains the `_all` string or no identifiers and there are no matches; contains wildcard expressions and there
are only partial matches.

If it is true, the API returns a successful acknowledgement message when there are no matches. When there are
only partial matches, the API stops the appropriate transforms.

If it is false, the request returns a 404 status code when there are no matches or only partial matches.
** *`force` (Optional, boolean)*: If it is true, the API forcefully stops the transforms.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response when `wait_for_completion` is `true`. If no response is received before the
timeout expires, the request returns a timeout exception. However, the request continues processing and
eventually moves the transform to a STOPPED state.
** *`wait_for_checkpoint` (Optional, boolean)*: If it is true, the transform does not completely stop until the current checkpoint is completed. If it is false,
the transform stops as soon as possible.
** *`wait_for_completion` (Optional, boolean)*: If it is true, the API blocks until the indexer state completely stops. If it is false, the API returns
immediately and the indexer is stopped asynchronously in the background.

[discrete]
==== update_transform
Updates certain properties of a transform.

{ref}/update-transform.html[Endpoint documentation]
[source,ts]
----
client.transform.updateTransform({ transform_id })
----

[discrete]
==== Arguments

* *Request (object):*
** *`transform_id` (string)*: Identifier for the transform.
** *`dest` (Optional, { index, op_type, pipeline, routing, version_type })*: The destination for the transform.
** *`description` (Optional, string)*: Free text description of the transform.
** *`frequency` (Optional, string | -1 | 0)*: The interval between checks for changes in the source indices when the
transform is running continuously. Also determines the retry interval in
the event of transient failures while the transform is searching or
indexing. The minimum value is 1s and the maximum is 1h.
** *`_meta` (Optional, Record<string, User-defined value>)*: Defines optional transform metadata.
** *`source` (Optional, { index, query, remote, size, slice, sort, _source, runtime_mappings })*: The source of the data for the transform.
** *`settings` (Optional, { align_checkpoints, dates_as_epoch_millis, deduce_mappings, docs_per_second, max_page_search_size })*: Defines optional transform settings.
** *`sync` (Optional, { time })*: Defines the properties transforms require to run continuously.
** *`retention_policy` (Optional, { time } | null)*: Defines a retention policy for the transform. Data that meets the defined
criteria is deleted from the destination index.
** *`defer_validation` (Optional, boolean)*: When true, deferrable validations are not run. This behavior may be
desired if the source index does not exist until after the transform is
created.
** *`timeout` (Optional, string | -1 | 0)*: Period to wait for a response. If no response is received before the
timeout expires, the request fails and returns an error.

